{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU9 - Regression: Learning notebook\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "- Simple Linear Regression\n",
    "- Gradient Descent\n",
    "- The impact of learning rate\n",
    "- Muliple Linear Regression\n",
    "- Using scikit learn linear regression\n",
    "- (Extra) Computational graphs: a conceptual framework for automated differentiation\n",
    "- (Extra) What is that *random_state* thing?\n",
    "\n",
    "\n",
    "## What is regression\n",
    "\n",
    "A modeling task which objective is to create a (linear or non-linear) map between the **independent variables** (i.e. the columns in your pandas dataframe) and a set of **continuous dependent variables** (i.e. the variable you want to predict) by estimating a set of **unknown parameters**. \n",
    "\n",
    "Examples of regression tasks:\n",
    "* predicting house prices (example range: [100k\\$; 500k\\$]);\n",
    "* predicting the rating that a user would assign to a movie (example range: [1 start; 7 stars]); \n",
    "* predicting emotional descriptors for a song;\n",
    "* predicting the trajectory of a fighter jet.\n",
    "\n",
    "Nowadays, there are *a lot* of algorithms to solve this task but we will focus on one of the most easy to understand: **linear regression**. It is one of the most used regression methods in the world to this day due to how easy it is to (1) interpret the model, (2) implement it and (3) implement extensions that deal with datasets with few data points, noise and outliers. \n",
    "\n",
    "First, let's explore how **simple linear regression** works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear regression\n",
    "\n",
    "This model is a special case of linear regression where you have a single feature. The model is, simply, a line equation\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$$\n",
    "\n",
    "* $\\hat{y}$ is the value predicted by the model; \n",
    "* $x$ is the input feature; \n",
    "* $\\beta_0$ is the y-axis value where $x=0$, usually called the *intercept*; \n",
    "* $\\beta_1$ tells you how much $\\hat{y}$ changes when $x$ changes, usually called the *coefficient*.\n",
    "\n",
    "Let's see what each parameter does in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import FloatSlider, Dropdown\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_regression(b0=0, b1=1, xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = b0 + b1 * x\n",
    "    \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.plot(x, y)\n",
    "    plt.plot([0, 0], ylim, 'g-', \n",
    "             xlim, [0, 0], 'g-', linewidth=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0fb69cbd01483694074c58b0a58b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_simple_regression, \n",
    "         b0=FloatSlider(min=-10, max=10, step=0.01, value=0), \n",
    "         b1=FloatSlider(min=-10, max=10, step=0.01, value=1), \n",
    "         xlim=fixed((-5, 5)), \n",
    "         ylim=fixed((-5, 5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green plot represents both the x and y axes while the blue line is the $\\hat{y}$ for each value of $x$. As you can see for yourself, if you decrease $\\beta_0$, the value where y TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to manually change $\\beta_0$ and $\\beta_1$ in order to fit a small dataset. In order to TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_regression_with_dataset(x, y, b0=0, b1=1, xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    plot_simple_regression(b0, b1, xlim, ylim)\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    y_hat = b0 + b1 * x\n",
    "    \n",
    "    return \"Mean Squared Error (MSE): {}\".format(mean_squared_error(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8fa5a9c69140e0bd943096d9c72810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = make_regression(n_features=1, n_samples=100, noise=30.5, random_state=10, bias=200)\n",
    "x = x[:, 0]\n",
    "y /= 100\n",
    "y *= 2.0\n",
    "\n",
    "interact(plot_simple_regression_with_dataset, \n",
    "         b0=FloatSlider(min=-10, max=10, step=0.01, value=-1), \n",
    "         b1=FloatSlider(min=-10, max=10, step=0.01, value=-1), \n",
    "         x=fixed(x), \n",
    "         y=fixed(y), \n",
    "         xlim=fixed((-5, 5)), \n",
    "         ylim=fixed((-3, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, doing this manually sucks. So, humans developed optimization algorithms to allow machines to adjust $\\beta_0$ and $\\beta_1$ according to some data set. There are, at least, 3 categories of optimization procedures to do it:\n",
    "\n",
    "1. iterative methods using gradients;\n",
    "2. closed form solution through normal equations;\n",
    "3. evolutionary methods like genetic algorithms or particle swarm.\n",
    "\n",
    "Methods based on 3 are kind of an overkill, they don't guarantee you the optimal set of parameters for the model and just a curiosity. Methods 1 and 2 are the ones that we actually use to optimize the parameters. Let's look into them. TODO\n",
    "\n",
    "\n",
    "TODO: em vez de explicar como se fazem as equações normais para optimizar os parâmetros, apontar os alunos para aqui:\n",
    "* https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "A lot of modern machine learning is based on setting up a really nice function and adjusting its internal parameters according to the data you have. In order to adapt the parameters of a function, you need somekind of optimization procedure. If your function is differentiable (TODO) and convex (TODO), you can find the best set of parameters using something called **gradient descent**. This refers to a whole family of optimization methods based on partial derivatives of functions. But, before going into what gradient descent actually is, we need to talk about **derivatives**!\n",
    "\n",
    "TODO: dar a analogia da descida por uma montanha\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "Derivatives is one of the main topics (if not the main topic) of a subfield of calculus called *differential calculus*. They allow you to know **how a small change in the input might change the output** of a differentiable function. The derivative of a function can be represented as another function that shows TODO\n",
    "\n",
    "\n",
    "### Back to Gradient Descent\n",
    "\n",
    "TODO\n",
    "\n",
    "For each observation in your dataset, update the parameters of your model with \n",
    "\n",
    "$$\\omega_{i+1} = \\omega_{i} - \\alpha \\frac{\\partial f}{\\partial \\omega_{i}}$$\n",
    "\n",
    "where $\\omega^{i}$ are the current values of your parameters. This optimization method can be used when TODO. \n",
    "\n",
    "Let's take a look at an example: imagine that the function we want to minimize is $f(x) = x^2$ and the current value of $x$ is -6. This means that each time you press the button *\"Run Interact\"*, $x$ will be updated using the following formula:\n",
    "\n",
    "$$x_{i+1} = x_{i} - \\alpha \\frac{\\partial f}{\\partial x_{i}} = x_{i} - \\alpha \\cdot 2 x_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfd9fc9308a4d72bdfab6c224852c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import run_sgd_step\n",
    "\n",
    "o = {'curr_x': -6.0}\n",
    "\n",
    "interact_manual(run_sgd_step, \n",
    "                learning_rate=FloatSlider(min=0.01, max=2.0, step=0.01, value=0.01), \n",
    "                o=fixed(o), \n",
    "                name=fixed('convex-1'), \n",
    "                range_def=fixed([-10, 10, 100000]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the *\"learning_rate\"* slider and try bigger values. In this case, if you set the learning_rate to something equal or above 1, we won't reach the global minima. So, for $f(x)=x^2$, if we keep $\\alpha < 1$, we will, sooner or later, get to the global minima.\n",
    "\n",
    "Now, let's make things a \"little\" bit harder. Let $f(x) = x^2 + \\left|15 x\\right| * cos(x)$ and the current value of $x$ be -6. For this new function $f(x)$, the next value of $x$ will be computed as \n",
    "\n",
    "$$x_{i+1} = \n",
    "x_{i} - \\alpha \\frac{\\partial f}{\\partial x_{i}} = \n",
    "x_{i} - \\alpha \\cdot (2 x_i + cos(x_i) \\cdot \\frac{15 x_i}{\\left| x_i \\right|} - \\left|15 x_i\\right| \\cdot sin(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sad_hamster](https://media.giphy.com/media/8UHwuM947LUjyyYh1o/giphy.gif \"I thought this was a bootcamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I know it looks like an awful complicated formula but bear with me. You will see why we used it in just a moment. So, let's use SGD with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b90a969f2f64d79b146cbcbb31be169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o = {'curr_x': -6.0}\n",
    "\n",
    "interact_manual(run_sgd_step, \n",
    "                learning_rate=FloatSlider(min=0.01, max=2.0, step=0.01, value=0.01), \n",
    "                o=fixed(o), \n",
    "                name=fixed('non-convex-1'), \n",
    "                range_def=fixed([-20, 20, 100000]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if $x$ starts at -6, we won't be able to get to the global minima using gradient descent. You could increase and/or decrease the learning rate in order to try to, get one of the global minima. But gradient descent TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using this abstract $\\omega$ parameter, let's use the linear regression symbols. Let the $f(x) = (y - \\hat{y})^2$, i.e. the squared error, and the updating rules\n",
    "\n",
    "Now, let's get back to simple linear regression. As we stated previously, we want to adjust $\\beta_0$ and $\\beta_1$ in order to use the regression model to make predictions for new values of $x$. TODO\n",
    "\n",
    "$$\\beta_{0_{i+1}} = \\beta_{0_{i}} - \\alpha \\frac{\\partial f}{\\partial \\beta_{0_{i}}} = $$\n",
    "$$\\beta_{1_{i+1}} = \\beta_{1_{i}} - \\alpha \\frac{\\partial f}{\\partial \\beta_{1_{i}}} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression_sgd_epoch(x, y, params, learning_rate): \n",
    "    data = np.concatenate((np.array([x]).T, np.array([y]).T), axis=1)\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for m in range(x.shape[0]):\n",
    "        x = data[:, 0]\n",
    "        y = data[:, 1]\n",
    "        \n",
    "        b0 = params['b0']\n",
    "        b1 = params['b1']\n",
    "\n",
    "        x_ = x[m]\n",
    "        y_ = y[m]\n",
    "        \n",
    "        y_hat = b0 + b1 * x_\n",
    "\n",
    "        d_mse_d_b1 = - 2 * (y_ - y_hat) * x_\n",
    "\n",
    "        d_mse_d_b0 = - 2 * (y_ - y_hat)\n",
    "\n",
    "        b0 = b0 - learning_rate * d_mse_d_b0\n",
    "        b1 = b1 - learning_rate * d_mse_d_b1\n",
    "    \n",
    "    params['b0'] = b0\n",
    "    params['b1'] = b1\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgd_for_1d(x, y, params, learning_rate):\n",
    "    run_linear_regression_sgd_epoch(x, y, params, learning_rate)\n",
    "    \n",
    "    x_ = np.linspace(-4, 4)\n",
    "    b0 = params['b0']\n",
    "    b1 = params['b1']\n",
    "    y_hat = b0 + b1 * x_\n",
    "    \n",
    "    plt.ylim([-4, 8])\n",
    "    plt.xlim([-6, 6])\n",
    "    plt.plot(x_, y_hat)\n",
    "    plt.scatter(x, y, c='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'b0': -1, \n",
    "    'b1': -5\n",
    "}\n",
    "\n",
    "run_sgd_for_1d(x, y, params, 0.05)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convex functions\n",
    "\n",
    "\n",
    "TODO: non-convex\n",
    "With non-convex functions, using gradient descent \n",
    "\n",
    "TODO: advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression_bgd_step(x, y, params, learning_rate): \n",
    "    b0 = params['b0']\n",
    "    b1 = params['b1']\n",
    "\n",
    "    d_mse_d_b1 = ((1 / m) * 2 * (y - y_hat) * x).sum()\n",
    "\n",
    "    d_mse_d_b0 = ((1 / m) * 2 * (y - y_hat)).sum()\n",
    "    \n",
    "    b0 = b0 - learning_rate * d_mse_d_b0\n",
    "    b1 = b1 - learning_rate * d_mse_d_b1\n",
    "    \n",
    "    params['b0'] = b0\n",
    "    params['b1'] = b1\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain why linear regression is a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Most phenomena in our world is dependent on several factors. For example, house prices depend on things like (1) number of rooms, (2) distance to malls, (3) distance to parks, (4) how old the house is, etc. As such, it would be naive to create a predictive linear mode\n",
    "\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_3 + \\beta_4 \\cdot x_4 + \\beta_5 \\cdot x_5$$\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^{5} \\beta_i \\cdot x_i$$\n",
    "\n",
    "$$\\hat{y}^{[j]} = \\beta_0 + \\sum_{i=1}^{5} \\beta_i \\cdot x_i^{[j]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: usually people interpret the coefficients as the **importance of the feature** within the model.\n",
    "TODO: the interpretation of the intercept depends on what a feature value of 0 means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_regression(n_features=1, n_samples=500, noise=30.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.concat((pd.DataFrame(x), pd.DataFrame(y)), axis=1)\n",
    "d.columns = ['x', 'y']\n",
    "d.plot.scatter('x', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty=None, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, lr.fit(x, y).predict(x), color='blue', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(10)\n",
    "x = rs.rand(5000, 1)\n",
    "y = rs.rand(5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, lr.fit(x, y).predict(x), color='blue', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm...the coeficient is pretty much close to zero and the intercept is about 0.5. \n",
    "\n",
    "Our completely random data set was generated, for both variables, between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit learn linear regression\n",
    "\n",
    "Scikit Learn is one of the main pieces of tech stacks for data science throughout the world. It offers a wide range of algorithms to create models for regression, classification and unsupervised learning tasks, as well as methods for preprocessing and visualizations. Also, it provides the users with well-thought abstractions to chain all the transformers and models into a single pipeline. TODO\n",
    "\n",
    "There are two implementations of the basic linear regression: \n",
    "* *sklearn.linear_model.SGDRegressor*: a multi-faceted class for regression tasks using linear models. The optimization procedure is the stochastic gradient descent.\n",
    "* *sklearn.linear_model.LinearRegression*: TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (EXTRA) What is that *random_state* thing?\n",
    "\n",
    "You have probably noticed that SGDRegressor used a misterious parameter: *random_state*. \n",
    "\n",
    "The parameter *random_state* is a very common one in scikit-learn and numpy. This parameter is the seed for the random numbers generator.\n",
    "\n",
    "This is actually **very important** for you to know because, by controlling the *random_state* value, you will make the entire process **reproducible** (i.e. everytime we run your code, we get the same results).\n",
    "\n",
    "You might be wondering _\"why does scikit-learn need to generate random numbers?\"_. Machine/Statistical learning and data analysis depend *a lot* on random processes. A random process depends, as the name suggests, on randomness. These random processes are used in many things: sampling probability distributions, initializing the parameters vector of a linear regression or neural network, selecting feature values to be used in cuts on decision trees, selecting subsets of data for cross-validation, etc. So, again, random_state is **really_important**.\n",
    "\n",
    "Inside every piece of scikit-learn code that uses random numbers generators TODO\n",
    "\n",
    "If you set the *random_state* parameter to an integer, you get the same result everytime, unless there is a bug in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(check_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_random_state(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = np.random.RandomState(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = check_random_state(np.random.RandomState(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (EXTRA) Computational graphs: a conceptual framework for automated differentiation\n",
    "\n",
    "Nowadays, many of the mainstream machine learning models are based on gradient descent in one way or another. Even if they aren't (explicitly), they are obtained through the minimization/maximization of a differentiable function (e.g. K-Means, Support Vector Machines, Probabilistic Graphical Models). \n",
    "\n",
    "Computational graphs aren't a theory that provides a better optimization method nor a better model. It is \"just\" the reframing of our mind set on how we represent functions. TODO\n",
    "\n",
    "MAIN PURPOSES OF CGs:\n",
    "* Make it easy to visually debug TODO\n",
    "* Unify several models, based on differentiable \n",
    "* TODO: shared parameters\n",
    "* TODO: max/min pool\n",
    "* A conceptual framework that allows efficient and parallelizable TODO\n",
    "\n",
    "\n",
    "Computational graphs are, literally, the backbone of frameworks like PyTorch, Autograd, TensorFlow and Chainer.\n",
    "\n",
    "In order to grasp the intuition behind Computational Graphs, let's start with some simple examples\n",
    "\n",
    "$$f(x) = x^2 + y + 2$$\n",
    "\n",
    "$$f(x) = x^2 + x y$$\n",
    "\n",
    "$$f(x) = \\frac{xy + zxy}{2x + y^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about CGs, we highly recommend you to read this [introduction to computational graphs and backpropagation][colah] by Christopher Olah.\n",
    "\n",
    "\n",
    "[colah]: http://colah.github.io/posts/2015-08-Backprop/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
