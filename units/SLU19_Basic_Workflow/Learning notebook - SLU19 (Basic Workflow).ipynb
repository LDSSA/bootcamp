{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU19 - Basic Workflow\n",
    "\n",
    "In this notebook we will be covering the following:\n",
    "\n",
    "- Notebook structure best practices\n",
    "- Programming tips\n",
    "- Basic pipelines\n",
    "- Reading portions of a file\n",
    "\n",
    "The goal here is to get some best practices established for basic projects using jupyter notebooks. More complicated projects will require more structure but for this academy these best practices should be enough.\n",
    "\n",
    "This learning unit will serve more as a reference and guideline provider rather than something to teach new technical material. This notebook will be mostly text and although you might not see the point of it at first, as you hit problems and find yourself wasting time on little (or big) things here and there, try to remember back to this and see if there's something in here to help get yourself organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why learn workflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you want to spend your time doing science! not debugging stupid little things constantly and wasting your time!\n",
    "\n",
    "Also, Data Science is largely an engineering discipline - you might as well just accept that right now. Writing code is an engineering practice and most data science is done with code these days. The most PURE industry research scientist I've spoken to works at Google's Deep Mind lab and he said that he is 50% software engineer!\n",
    "\n",
    "Also...\n",
    "\n",
    "## Jupyter is a terrible development environment\n",
    "\n",
    "Because it isn't one! A development environment is centered around being able to organize your code in an effective way. Jupter is made primarily for rapid prototyping and communication, not software engineering so there are going to be significant drawbacks when it comes to organizing your code and you will need to be extremely anal about following best practices because Jupyter won't do any of it for you the way that a real IDE would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports up top!\n",
    "\n",
    "You should always have your imports up at the top of the file. Anyone that looks at your notebook should get an idea of the tools that you are using IMMEDIATELY with a glance as soon as opening your notebook.\n",
    "\n",
    "And help your own sanity as well! You don't want to loose a key import during a cleanup and then when someone fires up your notebook for the first time and runs it, there's an import missing. This happens ALL the time, so you need to minimize as much as possible these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if you don't use an import until way later on in the file, put your imports here!\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize your directory\n",
    "\n",
    "1. All your notebooks should be in the same directory.\n",
    "1. You should create a data directory and put all of your data in it\n",
    "1. If you have images or other media, put it in a media directory.\n",
    "\n",
    "Here is an example directory structure from SLU17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('media/directory-structure.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When writing functions\n",
    "\n",
    "1. Keep them close to the cell where you are testing it at first\n",
    "1. When they are stabilizing, move them to the top of the notebook\n",
    "1. When you are starting to use them a lot, move them into a utils.py file and import from there\n",
    "\n",
    "You can see many examples of this in the files in this repository, follow them!\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need a new function called get_categoricals\n",
    "\n",
    "def get_categoricals(_df):\n",
    "    return _df.get_dtypes('categorical').columns\n",
    "\n",
    "# and I'm going to use it here\n",
    "\n",
    "cat_columns = get_categoricals(df)\n",
    "cat_columns_full_dataset = get_categoricals(df_full)\n",
    "cat_columns_medium_dataset = get_categoricals(df_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've finished with the function and it's all tested, I need to move the function up to the top of the file so that the cell where you are doing the actual work is much cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much cleaner, right?\n",
    "cat_columns = get_categoricals(df)\n",
    "cat_columns_full_dataset = get_categoricals(df_full)\n",
    "cat_columns_medium_dataset = get_categoricals(df_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I do this enough times, I'll get a notebook call that contains my functions up top that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'PassengerId',\n",
    "    'Name',\n",
    "    'Ticket',\n",
    "    'Sex',\n",
    "    'Cabin',\n",
    "    'Embarked',\n",
    "]\n",
    "\n",
    "# Here are a few functions that we will use to do our proof of concepts\n",
    "# around training and testing for kaggle submissions for the titanic\n",
    "# competition.\n",
    "\n",
    "# The reason that we need to have these functions they way they are\n",
    "# is to manage the preprocessing of the features the same for the \n",
    "# training and test set. Don't worry too much about exactly what's\n",
    "# going on in these functions right now but rather focus on the concepts\n",
    "# that are being covered after this in the notebook.\n",
    "\n",
    "\n",
    "def read_and_get_dummies(drop_columns=[]):\n",
    "    # when working inside of functions, always call your dataframe\n",
    "    # _df so that you know you're never using any from the outside!\n",
    "    _df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "    # now drop any columns that are specified as needing to be dropped\n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "    \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "    # Split the factors and the target\n",
    "    X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "    # take special note of this call!\n",
    "    X = pd.get_dummies(X, dummy_na=True).fillna(-1)\n",
    "\n",
    "    return _df, X, y\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(drop_columns=[], max_depth=None, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train a decision tree and return the classifier, the X_train,\n",
    "    and the original dataframe so that they can be used on the test\n",
    "    set later on.\n",
    "    \"\"\"\n",
    "    \n",
    "    _df, X, y = read_and_get_dummies(drop_columns=drop_columns)\n",
    "    \n",
    "    # Now let's get our train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=1, test_size=test_size)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    score = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print('X_test accuracy {}'.format(score))\n",
    "    print('X_train shape: {}'.format(X_train.shape))\n",
    "    \n",
    "    return X_train, _df, clf\n",
    "\n",
    "\n",
    "def produce_test_predictions(train_df, clf, drop_columns=[]):\n",
    "    _df = pd.read_csv('data/titanic-test.csv')\n",
    "    \n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "        \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null')\n",
    "        _df[colname] = pd.Categorical(\n",
    "            _df[colname],\n",
    "            categories=train_df[colname].cat.categories\n",
    "        )\n",
    "        \n",
    "    X = pd.get_dummies(_df, dummy_na=True).fillna(-1)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'PassengerId': pd.read_csv('data/titanic-test.csv').PassengerId,\n",
    "        'Survived': pd.Series(clf.predict(X))\n",
    "    })\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test Set score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAAAH! It's too much! I have to scroll for a mile just to get to the first informative part of the notebook! \n",
    "\n",
    "At this point, move it into a utils.py file\n",
    "\n",
    "## Caveat - changes to utils.py require a reload\n",
    "\n",
    "This is a huge bummer: jupyter will not automatically pick up on changes to your utils.py\n",
    "file without being told to reload them. This means that you should use the autoreload. The way you should use it is like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above your import cell, do this\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import Image\n",
    "from utils import say_hi\n",
    "\n",
    "# them in your import cell, include the %autoreload 2 at the bottom of it\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if you make changes to utils.py you just need to re-run the import cell and you are good to go with the updated versions of all your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodically restart your notebooks\n",
    "\n",
    "Because notebooks are continuously running processes, it's REALLY easy to leave stuff lying around that you can't see any more. I can't really show you an example in this notebook because it would require a live demo but let's do a little thought experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 of this cell\n",
    "a_var = 10\n",
    "b_var = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that I don't need the `b_var` any more so I delete it from the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 of this cell (imagine that I changed the original one)\n",
    "a_var = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with a notebook that doesn't have the `b_var` anywhere in it. However when I execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still here! Now this is sloppy and unfortunate and a perfect example of why juypter notebook is not an IDE. `b_var` does not exist anywhere in the notebook yet if I reference it, it still exists!\n",
    "\n",
    "The solution? Restart the notebook! If you do this and then run the `b_var` cell again, it will error as it should:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart and re-run the entire notebook before calling it done\n",
    "\n",
    "This is a huge one! Because of un-intended side effects of how jupyter works, it is inevitable that different cells will have been run with different versions of the notebook. If you have a cell working with an old version of a dataframe or function it could be displaying the wrong output. Then if you deliver this to someone, and they re-run it without changing ANY code, you're going to run into problems.\n",
    "\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello():\n",
    "    print('whaddap mah homie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the function and run it again in a different cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that if someone comes along and sees this they will be MASSIVELY confused. It's easy to fix this, just restart the notebook and run everything again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use functions!\n",
    "\n",
    "Don't be lazy with duplicating code. It is almost never worth it! The 5 minutes you will save by not taking common functionality and putting it into a function is almost never worth it because the confusion that comes afterward will always bite you.\n",
    "\n",
    "Let's take a real-life example from SLU17 in which we need to read a dataframe and get the dummies. However, sometimes we will want to drop columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the first case where we don't need to drop any columns\n",
    "\n",
    "_df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "for colname in categoricals:\n",
    "    if colname in drop_columns:\n",
    "        continue\n",
    "    _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember, we decided that there were useless features so we wanted to drop some. One way we could achieve this is to copy-paste the cell into another one and the modify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "##################\n",
    "# NEW CODE\n",
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "\n",
    "# now drop any columns that are specified as needing to be dropped\n",
    "for colname in drop_columns:\n",
    "    _df = _df.drop(colname, axis=1)\n",
    "# END NEW CODE\n",
    "##################\n",
    "\n",
    "for colname in categoricals:\n",
    "    if colname in drop_columns:\n",
    "        continue\n",
    "    _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "# Split the factors and the target\n",
    "X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "# take special note of this call!\n",
    "X = pd.get_dummies(X, dummy_na=True).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes my eyes bleed! Arrrggghhh it hurts! One of the main reasons this is such rough times is that is such rough times is imagine that you want to change the `fillna(-1)` to `fillna(0)`. You now have to change this in two places. Now imagine that you have duplicated this cell in 10 different places, you're going to have to remember to change this in 10 different places and you're going to have a terrible time.\n",
    "\n",
    "What's the right thing to do here? Put it in a function! It doesn't have to be anything special and as you all passed the test, we know you all know how to write function so NO EXCUSES. Here's what my function for this ended up looking like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_get_dummies(drop_columns=[]):\n",
    "    # when working inside of functions, always call your dataframe\n",
    "    # _df so that you know you're never using any from the outside!\n",
    "    _df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "    # now drop any columns that are specified as needing to be dropped\n",
    "    for colname in drop_columns:\n",
    "        _df = _df.drop(colname, axis=1)\n",
    "    \n",
    "    for colname in categoricals:\n",
    "        if colname in drop_columns:\n",
    "            continue\n",
    "        _df[colname] = _df[colname].fillna('null').astype('category')\n",
    "\n",
    "\n",
    "    # Split the factors and the target\n",
    "    X, y = _df.drop('Survived', axis=1), _df['Survived']\n",
    "\n",
    "    # take special note of this call!\n",
    "    X = pd.get_dummies(X, dummy_na=True).fillna(-1)\n",
    "\n",
    "    return _df, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would take the previous two cells to this niceness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, X, y = read_and_get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    'Ticket',\n",
    "    'PassengerId'\n",
    "]\n",
    "df_train, X, y = read_and_get_dummies(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, huge improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When writing functions\n",
    "\n",
    "1. If you take a parameter that is a dataframe, always name if `_df`\n",
    "1. Outside of functions, never name a dataframe `_df`\n",
    "1. Inside of functions, never name a dataframe `df`\n",
    "\n",
    "Why you ask? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you're working in a notebook with the following dataframe\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})\n",
    "# say you have a convenience function to drop all object columns\n",
    "def drop_cats(df):\n",
    "    return df.drop(df.select_dtypes('object').columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  f_1  f_2\n",
       "0   50    1    4\n",
       "1   60    2    5\n",
       "2   70    3    6\n",
       "3   80    4    7"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you make another version of the function because you remember to name the dataframe parameter `_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  f_1  f_2\n",
       "0   50    1    4\n",
       "1   60    2    5\n",
       "2   70    3    6\n",
       "3   80    4    7"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_and_dogs = pd.DataFrame({\n",
    "    'cat': ['hello'],\n",
    "    'dog': [1]\n",
    "})\n",
    "\n",
    "def drop_cats(_df):\n",
    "    return df.drop(df.select_dtypes('object').columns, axis=1)\n",
    "\n",
    "drop_cats(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah this was not expected! Easy fix though: when working on functions, always use `_df` from the very beginning when inside of the function and then never use `_df` as a variable name outside of the function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability is key!\n",
    "\n",
    "If you pass a dataframe into a function, NEVER EVER EVER will you want to change the dataframe that was passed in. This causes chaos in the world. A few rules to adhere to when doing this are the following:\n",
    "\n",
    "1. Never use inplace=True\n",
    "1. If you have a function that takes a dataframe as an argument and returns a modified version of it, always copy it at the beginning of the function and make your changes to that copy.\n",
    "\n",
    "Why? Let's see an example.\n",
    "\n",
    "Say I want a function that drops all columns that start with 'f_':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_f(_df):\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for colname in _df.columns:\n",
    "        if colname.startswith('f_'):\n",
    "            cols_to_drop.append(colname)\n",
    "\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's drop the f_\n",
    "df_no_f = drop_f(df)\n",
    "df_no_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great success! Now let's check out the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, we never said to drop it from the original dataframe but it happened anyway! What the heck?\n",
    "\n",
    "That's because we violated the rules and use inplace=True nor did we make a copy! Let's rewrite this function to be a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_f(_df):\n",
    "    \n",
    "    _df = _df.copy()\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    for colname in _df.columns:\n",
    "        if colname.startswith('f_'):\n",
    "            cols_to_drop.append(colname)\n",
    "\n",
    "    _df = _df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_f = drop_f(df)\n",
    "df_no_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "Now at this point you might be saying: why not just pick one of the two rules, either of those would have worked. That's totally true, in this case. However, all good programmers should adhere to best practices and the problem with `inplace=True` is that it changes the `drop()` API by making it return `None`.\n",
    "\n",
    "This WILL CAUSE CONFUSTION! If you are used to using `inplace=True` and another person you are working with is not used to using it, I can guarantee that you'll copy the person's code at least once and then wonder why this doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'f_1': [1, 2, 3, 4],\n",
    "    'f_2': [4, 5, 6, 7],\n",
    "    'age': [50, 60, 70, 80],\n",
    "    'sex': ['m', 'f', 'f', 'm']\n",
    "})\n",
    "no_f = df.drop('f_1', inplace=True, axis=1)\n",
    "no_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is for these reasonas well as many others that immutability is preferable not matter what language you're working in. Trust me on this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit pipelines\n",
    "\n",
    "Sooner than later, you will run into the problem of having to do duplicate pre-processing for a training and a test set or for different folds in cross validation. This can be a huge pain the the butt and can result in duplicated code or functions that have a crazy amount of arguments. \n",
    "\n",
    "Let's start of with a bit of motivation by looking at the titanic dataset where we will drop all categorical features and fill the nulls on the rest with the median\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic.csv')\n",
    "X_train, y_train = train_df.drop('Survived', axis=1), train_df.Survived.copy()\n",
    "X_test = pd.read_csv('data/titanic-test.csv')\n",
    "\n",
    "# now let's preprocess and train\n",
    "\n",
    "X_train_clean = X_train.select_dtypes(exclude='object').copy()\n",
    "# note that you will want to impute with the median age from the training set\n",
    "# and NOT the test set. This creates a few difficultites when trying to design\n",
    "# around it\n",
    "X_train_clean['Age'] = X_train_clean.Age.fillna(X_train_clean.Age.median())\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_clean, y_train)\n",
    "\n",
    "# then to test, we will need to do the same set of preprocessing\n",
    "\n",
    "X_test_clean = X_test.select_dtypes(exclude='object').copy()\n",
    "X_test_clean['Age'] = X_test_clean.Age.fillna(X_train_clean.Age.median())\n",
    "# now it turns out that X_test_clean has a column with nulls that X_test\n",
    "# didn't have so the preprocessing would have to be a bit different\n",
    "\n",
    "# Now there are some nulls in Fare for the test set that were not \n",
    "# in the training set.\n",
    "X_test_clean['Fare'] = X_test_clean.Fare.fillna(X_train.Fare.median())\n",
    "\n",
    "preds = clf.predict_proba(X_test_clean)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's totally true that we could write a few functions to take care of this, but scikit already provides some tooling that end up being cleaner using [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "What is a pipeline? Pretty simple: it's a set of steps that has a model at the end of it. It implements the same API as the models (has `predict` and/or `predict_proba`) but it applies each of the steps before calling the model with the input!\n",
    "\n",
    "Let's see how to use one to simplify the code we just looked at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic.csv')\n",
    "X_train, y_train = train_df.drop('Survived', axis=1), train_df.Survived.copy()\n",
    "X_test = pd.read_csv('data/titanic-test.csv')\n",
    "\n",
    "# for some cases, we will want to create our own pipeline step\n",
    "# this provides a lot of flexibility!\n",
    "class RemoveColumns(TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return X.select_dtypes(exclude='object').copy()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "# now let's make the pipeline\n",
    "pipeline = make_pipeline(\n",
    "    RemoveColumns(),\n",
    "    # it's cool how scikit already has a mean imputer ready to go!\n",
    "    Imputer(strategy='mean'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# No need for us to manually preprocess the X_train at all\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Same thing for X_test\n",
    "probas = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can take a bit of time to learn how to use and they do have some strange behavior in some cases so be a bit patient with them - the work pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handeling large datasets on your machine\n",
    "\n",
    "Okay say that your working with a fairly large dataset and it's taking a while to train. Furthermore may the full dataset doesn't fit into memory on your machine but you still want to do some EDA as well as train a model. There are a few ways to do this\n",
    "\n",
    "1. Only read part of the file into memory\n",
    "1. If you are able to read the whole thing into memory but don't want to be working with it the entire time, you can take a sample of it while doing your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only read the first 10 rows\n",
    "pd.read_csv('data/titanic.csv', nrows=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 12)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the whole thing into memory but then sample only a portions of it\n",
    "pd.read_csv('./data/titanic.csv').sample(frac=.10).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
