{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU9 - Regression: Learning notebook\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "    - What is regression?\n",
    "    - Simple Linear Regression\n",
    "    - Gradient Descent\n",
    "    - Multiple Linear Regression\n",
    "    - Using Scikit Learn to perform regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is regression? \n",
    "\n",
    "A modeling task which objective is to create a (linear or non-linear) map between the **independent variables** (i.e. the columns in your pandas dataframe) and a set of **continuous dependent variables** (i.e. the variable you want to predict) by estimating a set of **parameters**. \n",
    "\n",
    "Examples of regression tasks:\n",
    "* predicting house prices (example range: [100k\\$; 500k\\$]);\n",
    "* predicting the rating that a user would assign to a movie (example range: [1 start; 7 stars]); \n",
    "* predicting the total sales for each day, in each shop of a shopping mall;\n",
    "* predicting emotional descriptors for a song;\n",
    "* predicting the trajectory of a fighter jet.\n",
    "\n",
    "Let's align the first 3 terms we used with an example: when predicting house prices, \n",
    "* the **independent variables** are the features related to the house like the number of rooms, the total area available, the crime rate of the area, the number of miles to the closest big city; \n",
    "* the **dependent variable** will be the price of the house.\n",
    "\n",
    "To predict the house prices, the regression model will take the independent variables and a set of internal parameters, perform a sequence of operations with those two and output the price of the house.\n",
    "\n",
    "Nowadays, there are *a lot* of algorithms to solve this task but we will focus on one of the easiest to understand: **linear regression**. It is one of the most used regression methods in the world to this day due to how easy it is to (1) interpret the model, (2) implement it and (3) implement extensions that deal with datasets with few data points, noise, and outliers. \n",
    "\n",
    "First, let's explore how **simple linear regression** works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "This model is a special case of linear regression where you have a single feature. The model is, simply, a line equation\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$$\n",
    "\n",
    "* $\\hat{y}$ is the value predicted by the model; \n",
    "* $x$ is the input feature; \n",
    "* $\\beta_0$ is the y-axis value where $x=0$, usually called the *intercept*; \n",
    "* $\\beta_1$ tells you how much $\\hat{y}$ changes when $x$ changes, usually called the *coefficient*.\n",
    "\n",
    "The impact of $x$ in $\\hat{y}$ can be state as the following: _For each unit you increment in $x$, you increment $\\beta_1$ units in $\\hat{y}$._ For example: \n",
    "\n",
    "$$HousePrice = 1.1 + 4 \\cdot NumberOfRooms$$\n",
    "\n",
    "means that the _price of the house increments 4 units (e.g. 1 unit = 10000$) each time I add a room to the house._\n",
    "\n",
    "You can create a simple lambda function in order to implement this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = lambda x, b0, b1: b0 + b1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some data and test this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(x, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(x, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10,  -7,  -4,  -1,   2,   5,   8,  11,  14,  17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(x, -10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to manipulate the model parameters, let's use the following demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_linear_regression_manual_demo_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcd53e0800c4f10a4b04f21d3c5fc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_linear_regression_manual_demo_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green plot represents both the x and y-axes while the blue line is the $\\hat{y}$ for each value of $x$. As you can see for yourself, if you decrease/increase $\\beta_0$, the value where y cross $\\hat{y}$ decreases/increases. If you increase/decrease $\\beta_1$, the slope of the line increases/decreases.\n",
    "\n",
    "Now, let's try to manually change $\\beta_0$ and $\\beta_1$ in order to fit a small dataset. In order to make your job easier, we added a metric (let's call it $J$) that goes down when you use better parameter combinations\n",
    "\n",
    "$$J(y, \\beta, x) = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 = \\frac{1}{N} \\sum_{n=1}^N (y_n - (\\beta_0 + \\beta_1 \\cdot x_n))^2$$\n",
    "\n",
    "where $y_n$ is the target $n$ in your dataset. To implement this cost function, we can have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_cost = lambda y, y_hat: ((y - y_hat)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_linear_regression_demo_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708fbc12b86445ce829903d1b1711c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_linear_regression_demo_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, doing this manually sucks. So, humans developed optimization algorithms to allow machines to adjust $\\beta_0$ and $\\beta_1$ according to some dataset. There are, at least, 3 categories of optimization procedures to do it:\n",
    "\n",
    "1. Iterative methods using gradients;\n",
    "2. Closed form solution through normal equations;\n",
    "3. Evolutionary methods like genetic algorithms or particle swarm; \n",
    "4. Bayesian optimization.\n",
    "\n",
    "Methods based on 3 and 4 are kind of an overkill at this point in time. We will explore methods based on gradient descent because they provide a, somehow, universal approach to optimization tasks and are really simple to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a well known and studied method for iterative optimization of both linear and non-linear models. You can use it to estimate the parameters for linear regression, neural networks, probabilistic graphical models, k-means and many more!\n",
    "\n",
    "The essential component of the gradient descent algorithm is the **update rule**. Let $f$ be a differentiable function and $\\omega$ one of parameters of $f$. Then, in order to minimize the value outputted by $f(\\omega)$, we will use, iteratively, the following\n",
    "\n",
    "$$\\omega = \\omega - \\alpha \\frac{\\partial f(\\omega)}{\\partial \\omega}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\frac{\\partial f}{\\partial \\omega}$ is the [partial derivative of $f$ with respect to $\\omega$](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) and $\\alpha$ is the learning rate. So, what gradient descent does is using the partial derivative as a _heuristic_ to the direction where the minimum is located and the multiplication between the learning rate and the partial derivative gives you a _velocity_ factor that you will use in order to update $\\omega$. There are two ways to increase the _velocity_: (1) higher learning rates, (2) big gradients. \n",
    "\n",
    "In the following demo, you will control the learning rate of gradient descent for the minimization of $f(x) = x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gradient_descent_learning_rate_impact_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b00e0ef1c44718b736bfb4f6b28967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.001, description='learning_rate', max=1.1, min=0.001, step=0.001), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient_descent_learning_rate_impact_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening in the demo above is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-60.0, 3600.0, -600)\n",
      "(-12.0, 144.0, -120.0)\n",
      "(-2.3999999999999986, 5.759999999999994, -24.0)\n",
      "(-0.47999999999999954, 0.23039999999999955, -4.799999999999997)\n",
      "(-0.09599999999999986, 0.009215999999999974, -0.9599999999999991)\n",
      "(-0.019199999999999967, 0.00036863999999999875, -0.19199999999999973)\n",
      "(-0.0038399999999999927, 1.4745599999999944e-05, -0.038399999999999934)\n",
      "(-0.0007679999999999983, 5.898239999999973e-07, -0.0076799999999999854)\n",
      "(-0.00015359999999999961, 2.3592959999999883e-08, -0.0015359999999999966)\n",
      "(-3.071999999999992e-05, 9.437183999999953e-10, -0.00030719999999999923)\n"
     ]
    }
   ],
   "source": [
    "# Define the function to be minimized.\n",
    "f = lambda x: x ** 2\n",
    "\n",
    "# Define the partial derivative of the \n",
    "# function with respect to the feature.\n",
    "df_dx = lambda x: 2 * x\n",
    "\n",
    "# Define the learning rate.\n",
    "learning_rate = 0.4\n",
    "\n",
    "# The initial value for x.\n",
    "x = -300\n",
    "\n",
    "# The number of iterations to use.\n",
    "epochs = 10\n",
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(epochs): \n",
    "    # Compute the derivative of f.\n",
    "    deriv = df_dx(x)\n",
    "    \n",
    "    # Update x.\n",
    "    x = x - learning_rate * deriv\n",
    "    \n",
    "    print((x, f(x), deriv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you increase the learning rate, it will converge faster to 0. But if you keep increasing to above 0.75-0.80, you will start to see a slower convergence or, even worst, overshooting (i.e. the value of $x$ gets to $-\\infty$ or $+\\infty$). This function was quite easy to minimize because it is a [convex function](https://en.wikipedia.org/wiki/Convex_function). But gradient descent has issues with non-convex functions. To dive into those issues, check the bonus material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Batch Gradient Descent (BGD)**\n",
    "\n",
    "Unlike SGD, BGD aggregates the partial derivatives from **all observations in the dataset** before using the update rule\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    2. $\\omega = \\omega + \\alpha \\sum_{n=1}^N \\frac{\\partial f(x_n, \\omega)}{\\partial \\omega}$\n",
    "    \n",
    "The main advantage of this approach is that you get a smoother path to the minimum, with less variance in the path to the minimum. Translating the generic formula to the context of simple linear regression:\n",
    "\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. $\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0} = \\beta_0 + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 (y_n - \\hat{y}_n)$ \n",
    "    2. $\\beta_1 = \\beta_1 - \\alpha \\frac{\\partial J}{\\partial \\beta_1} = \\beta_1 + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 (y_n - \\hat{y}_n) x_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd_for_simple_linear_regression(x, y, b0, b1, learning_rate, epochs):\n",
    "    # For an arbitrary number of epochs\n",
    "    for epoch in range(epochs): \n",
    "        # Compute predictions.\n",
    "        y_hat = linear_regression(x, b0, b1)\n",
    "        \n",
    "        # Compute the partial derivatives of J.\n",
    "        dJ_db0 = -(2 * (y - y_hat)).mean()\n",
    "        dJ_db1 = -(2 * (y - y_hat) * x).mean()\n",
    "        \n",
    "        # Update the intercept and coefficient.\n",
    "        b0 = b0 - learning_rate * dJ_db0\n",
    "        b1 = b1 - learning_rate * dJ_db1\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have a demo for BGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5837ab8704e44ed1b61c628f064f9141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', min=0.01, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import bgd_simple_lr_dataset_demo\n",
    "\n",
    "bgd_simple_lr_dataset_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROS**\n",
    "* It is less sensitive to the learning rate than Stochastic Gradient Descent (SGD, check the bonus materials). For that reason, you can use a bigger learning rate in order to get faster to the minimum.\n",
    "* For smooth error curves/surfaces and datasets not so big, you can get updates on your parameters without having to take a break for lunch, for a snack, for Netflix, for dinner...\n",
    "\n",
    "**Cons**\n",
    "* It is **really expensive** for big datasets to get an update on your parameters because you need to go through **all the dataset** in order to perform an update.\n",
    "\n",
    "If you already know both BGD and SGD, you might be wondering: *\"Can't we have some middle ground between SGD and BGD?\"*. Well, the answer is: **YES!!**. But that is an advanced topic! Once you manage to grasp the idea of gradient descent, SGD, and BGD, you are free to go explore more advanced methods like the Mini-Batch (Stochastic) Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Most phenomena in our world is dependent on several factors. For example, house prices depend on things like (1) number of rooms, (2) distance to malls, (3) distance to parks, (4) how old the house is, etc. As such, it would be naive, at best, to create a univariate linear model to predict the house prices. So, let's expand our simple linear regression into *multiple* linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_3 + \\beta_4 \\cdot x_4 + \\beta_5 \\cdot x_5 = \\hat{y} = \\beta_0 + \\sum_{i=1}^{5} \\beta_i \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_linear_regression = lambda x, betas: betas[0] + x.dot(betas[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple, hein? The more features you have, the more you include in the sum! But there is a **very important assumption** that this model does: there is no multicollinearity in your data. _\"What the hell does that even mean?\"_, you might ask. This term comes from linear algebra and this is what it means: \n",
    "\n",
    "_If you have a feature $x_i$ which the values can be obtained through a **linear combination** of other features, then we have multicollinearity._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me give you some of the reasons this is a problem: \n",
    "1. When people use linear regressions, after the parameter estimation phase, they use the coefficients as a way to measure **how important a feature**. When you use collinear features, the magnitude of the weights gets 's lowered for all features that are in that collinear relationship. That might be misleading because collinear features are, essentially, one feature.\n",
    "2. Collinear features add no value to the model, it is like cloning the same feature and concatenating it to the dataset. \n",
    "\n",
    "We will experiment with a dataset with collinear features later on this learning unit.\n",
    "\n",
    "Also, you should always normalize your dataset into a unified scale (e.g. range [0; 1]). The reasons why:\n",
    "1. Depending on what optimization algorithm you use, if feature $f_1$ has a domain of [-4.1; 3] and feature $f_2$ has a domain of [-1.1; 100000], the impact in the gradient can lead to problems in the convergence to the global minima (i.e. you probably won't get accurate results for your predictions). There are optimization algorithms that can avoid this issue but still suffer from the issue (2).\n",
    "2. If two features are using different ranges, it will be hard to compare features in terms of feature importance. If a feature ($f_1$) has a domain of [-0.1; 0.1] and another ($f_2$) has domain of [0; 1000], it doesn't make sense to look at the influence in the prediction through the same lens as the ones we use in the introduction to simple linear regression (i.e. an increase of 1 unit in $x$ increases $\\beta_1$ units in $\\hat{y}$). Due to the fact that feature $f_1$ never goes to -1 or +1, that interpretation wouldn't make sense and, as such, we would have two options:\n",
    "    1. normalize the resulting coefficients accordingly to the scale;\n",
    "    2. or normalize the inputs into the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use gradient descent to optimize the intercept and coefficients. First, let's extract the partial derivatives of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_0} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_1} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_1} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) x_{1_n} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_k} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_k} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) x_{k_n} $$\n",
    "\n",
    "\n",
    "\n",
    "where $x_{1_n}$ is the value of feature 1 for the instance $n$ of the dataset and $x_{k_n}$ is the value of feature $k$ for the instance $n$ of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BGD, we have\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. $\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0} = \\beta_0 + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 (y_n - \\hat{y}_n)$ \n",
    "    2. _For i in 1..K:_\n",
    "        1. $\\beta_i = \\beta_i - \\alpha \\frac{\\partial J}{\\partial \\beta_i} = \\beta_i + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 (y_n - \\hat{y}_n) x_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd_multiple_linear_regression(x, y, betas, learning_rate, epochs):\n",
    "    for epoch in range(epochs): \n",
    "        y_hat = multiple_linear_regression(x, betas)\n",
    "\n",
    "        dJ_dbetas = np.zeros((x.shape[1] + 1, 1))\n",
    "\n",
    "        dJ_dbetas[0] = -(2 * (y - y_hat)).mean()\n",
    "\n",
    "        for k, col in enumerate(x.columns): \n",
    "            dJ_dbetas[k+1] = -((2 * (y - y_hat)) * x[col]).mean()\n",
    "    \n",
    "    return dJ_dbetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Pros & Cons\n",
    "\n",
    "**PROS**\n",
    "* Really easy to understand\n",
    "* Fast optimization\n",
    "* Easy to extend the model: you haven't seen how this is true but there is a thing called _Generalized Linear Model (GLM)_. Once you get a good grip on linear regression, you should visit the [scikit learn page on GLMs](http://scikit-learn.org/stable/modules/linear_model.html).\n",
    "\n",
    "**CONS**\n",
    "* Sensible to outliers, even though there are extensions that are able to deal (partially) with this issue.\n",
    "* Assumes that there is no multicollinearity.\n",
    "* Feature scaling is required.\n",
    "* Monotonicity assumption: for the model, the relation between each feature and the output. \n",
    "* Categorical encoding: this might get tricky when the number of uniques is big and part of those uniques have few occurrences.\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "At this point, if you already knew linear regression in detail before the academy, you might be wondering: *\"Where is the error component in the linear regression formula?\"*. The reason is quite simple: since we wanted you to approach this subject from a more practical standpoint than theoretical.\n",
    "\n",
    "Also, we didn't include all assumptions made by the linear regression model. For a hands-on approach to the assumptions, check this [blog post by Selva Prabhakaran](http://r-statistics.co/Assumptions-of-Linear-Regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn to perform regression\n",
    "\n",
    "After learning the basics of linear regression and how to estimate, iteratively, the best parameters for the model, it is time to learn how to use linear regression with Scikit Learn. \n",
    "\n",
    "[Scikit Learn][sklearn] is an industry standard for data science and machine learning and we will be using it extensively throughout the academy. Scikit Learn has two implementations of linear regression:\n",
    "* [*sklearn.linear_model.SGDRegressor*][SGDRegressor]: uses stochastic gradient descent to estimate the intercept and coefficients. Also, this class allows more advanced forms of linear regression that is out of scope for the moment.\n",
    "* [*sklearn.linear_model.LinearRegression*][LinearRegression]: uses normal equations to estimate the best intercept and coefficients. Normal equations are the closed form solution for linear regression, meaning that you know exactly the number of steps and the guarantees about the solution. If you want to know more about this, [read this blog post][normal_eq].\n",
    "\n",
    "[SGDRegressor]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "[LinearRegression]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "[sklearn]: http://scikit-learn.org\n",
    "[normal_eq]: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the [Boston housing price dataset][boston_kaggle]\n",
    "\n",
    "[boston_kaggle]: https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "print(data['DESCR'])\n",
    "\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.Series(data['target'], name='medv')\n",
    "\n",
    "pd.concat((X, y), axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with the first linear regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(X.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an R² score of ~74, which might be adequate for the first try. R² is one of the most well-known metrics to evaluate regression models.We will dive into it in SLU12.\n",
    "\n",
    "Modeling is not only about getting the best accurate model ever. If you get a big R² score for the wrong reasons (e.g. target leaks, too many useless variables), that model is kind of...useless. As such, let's look into how each feature contributes to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOX       -17.795759\n",
       "RM          3.804752\n",
       "CHAS        2.688561\n",
       "DIS        -1.475759\n",
       "PTRATIO    -0.953464\n",
       "LSTAT      -0.525467\n",
       "RAD         0.305655\n",
       "CRIM       -0.107171\n",
       "ZN          0.046395\n",
       "INDUS       0.020860\n",
       "TAX        -0.012329\n",
       "B           0.009393\n",
       "AGE         0.000751\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=X.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOX_, according to the dataset documentation, refers to _\"nitric oxides concentration (parts per 10 million)\"_. The coefficient for _NOX_ is WAAAAAY BIGGER than the ones in the other features. Does it mean that (1) air pollution is a BIIIG problem in Boston, (2) people that buy houses in Boston REALLY REALLY REALLY HATE pollution\n",
    "\n",
    "or does it mean that something was wrong in our approach? \n",
    "\n",
    "First of all, let's check some statistics about our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that there the scales for different features are *way* different from one another. For example, the domain of _CRIM_ is [0.006320; 88.976200] while _TAX_ is in [187; 711]. This means that, in the context of linear regression, the **coefficients are not comparable**. Fortunately, we have a preprocessed version of this dataset with all features in the same scale. Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649427\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "\n",
    "X_ = data.drop(['MEDV'], axis=1)\n",
    "y = data['MEDV']\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(X_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(X_, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.748680\n",
       "DIS       -3.104448\n",
       "RM         2.670641\n",
       "RAD        2.658787\n",
       "TAX       -2.075898\n",
       "PTRATIO   -2.062156\n",
       "NOX       -2.060092\n",
       "ZN         1.080981\n",
       "CRIM      -0.920411\n",
       "B          0.856640\n",
       "CHAS       0.682203\n",
       "INDUS      0.142967\n",
       "AGE        0.021121\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=X_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can normalize the coefficients in order to see the relative weight of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT      0.169739\n",
       "DIS        0.140568\n",
       "RM         0.120925\n",
       "RAD        0.120389\n",
       "TAX        0.093996\n",
       "PTRATIO    0.093373\n",
       "NOX        0.093280\n",
       "ZN         0.048946\n",
       "CRIM       0.041676\n",
       "B          0.038788\n",
       "CHAS       0.030890\n",
       "INDUS      0.006473\n",
       "AGE        0.000956\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.abs() / a.abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that over 50% of the relative feature strength is concentrated in: \n",
    "* _LSTAT_ (decreases price): % lower status of the population\n",
    "* _DIS_ (decreases price): weighted distances to five Boston employment centers\n",
    "* *RM* (increases price): average number of rooms per dwelling\n",
    "* _RAD_ (increases price): index of accessibility to radial highways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to use SGDRegressor. As previously stated, this class allows fine tuning regarding learning rate, weights constraints, extensions of gradient descent, etc. We will use the configuration that allows the most similar behavior to the one described for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [-8.81456209e+13 -9.34010426e+13 -8.94837901e+13 -8.95555858e+13\n",
      " -9.13474538e+13]\n",
      "\n",
      "Total Score\n",
      " -5.973466231191271e+25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(X.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF?! We got prediction overshooting and an AWFUL R² score! Is this related to feature scaling? Let's check if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [29.69419554 24.75456216 30.16659133 28.09211046 27.46078478]\n",
      "\n",
      "Total Score\n",
      " 0.7380980597701932\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(X_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(X_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(X_, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess we have our answer. Unlike _LinearRegression_, **having the same scale is not an option** for _SGDRegressor_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.680366\n",
       "DIS       -3.181424\n",
       "RM         2.634311\n",
       "RAD        2.633794\n",
       "PTRATIO   -2.035668\n",
       "NOX       -1.982095\n",
       "TAX       -1.931509\n",
       "ZN         1.031784\n",
       "CRIM      -0.891170\n",
       "B          0.809772\n",
       "CHAS       0.694754\n",
       "INDUS      0.168438\n",
       "AGE        0.087407\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=X_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore the effect of **multicollinearity**. First, let's create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=4, n_informative=4, \n",
    "                       n_targets=1, random_state=10, noise=20)\n",
    "x = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a copy of $X$ and include multicollinearity in the new copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X.copy()\n",
    "\n",
    "X_[4] = 0.3 * X_[0] + 0.3 * X_[1] + 0.4 * X_[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a linear regression with the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score:  0.973873466791004\n",
      "Coefficients:  [95.89264211 20.7771769  67.332997   61.57570178]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print('R² score: ', lr.score(X, y))\n",
    "print('Coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one with the copy of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score:  0.931420777936116\n",
      "Coefficients:  [93.15355813 21.11015385 65.49562433 59.11102235]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_, y)\n",
    "print('R² score: ', lr.score(X_, y))\n",
    "print('Coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERESTING!!!** It seems that the magnitude of features *f0*, *f1* and *f2* was \"transferred\" to *f4*! But the R² score is the same! \n",
    "\n",
    "Do we get the same type of issue with SGDRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score:  0.9738714924029974\n",
      "Coefficients:  [95.81397888 20.83793972 67.21773658 61.44690935]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(X, y)\n",
    "print('R² score: ', lr.score(X, y))\n",
    "print('Coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score:  0.9314192859901802\n",
      "Coefficients:  [93.09869394 21.1590517  65.39629399 58.99812664]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(X_, y)\n",
    "print('R² score: ', lr.score(X_, y))\n",
    "print('Coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got the same type of results, with the exception of the small R² score oscillation (due to stochastic process influence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps we didn't include: \n",
    "* Exclude the feature _CHAS_ from the scaler. _CHAS_ is a dummy feature (i.e. the result of categorical feature encoding) and isn't a continuous feature per se.\n",
    "* Perform correlation analysis in order to avoid including 2, or more, features that are highly correlated. When using highly correlated features in a linear model, you might be violating the assumption of no multicollinearity.\n",
    "* We didn't remove outliers. This is a problem for models like linear regression due to sensitivity to outliers. Fortunately, there are implementations for robust linear regression within scikit learn ([RANSAC][RANSAC], [Theil-Sen][Theil-Sen] and [Huber][Huber]).\n",
    "* We didn't perform correlation analysis between each input feature and the target. \n",
    "\n",
    "We didn't include those steps but, with all you have learned so far in the academy, you are able to perform those by yourself. :)\n",
    "\n",
    "\n",
    "[RANSAC]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor\n",
    "[Theil-Sen]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor\n",
    "[Huber]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent & Non-Convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the effect of non-convex functions in gradient descent, let's minimize $x$ in the following function\n",
    "\n",
    "\n",
    "$$f(x) = x^2 + \\left|15 x\\right| * cos(x)$$\n",
    "\n",
    "it will be minimized with\n",
    "\n",
    "$$x_{i+1} = \n",
    "x_{i} - \\alpha \\frac{\\partial f}{\\partial x_{i}} = \n",
    "x_{i} - \\alpha \\cdot (2 x_i + cos(x_i) \\cdot \\frac{15 x_i}{\\left| x_i \\right|} - \\left|15 x_i\\right| \\cdot sin(x_i))$$\n",
    "\n",
    "\n",
    "After looking at the previous formula, you might be thinking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sad_hamster](assets/hamster.gif \"I thought this was a bootcamp\")\n",
    "\n",
    "**I KNOW, YOU WERE ALL TRICKED!!! MUAHAHAHAHAH**\n",
    "\n",
    "Relax, that function is being used in order to show you some of the issues with gradient descent. Yes, I know it looks like an awfully complicated formula but bear with me. You will see why we used it in just a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fe4a0e2dd94afe984c6979f8cf964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import non_convex_gradient_descent_demo\n",
    "\n",
    "non_convex_gradient_descent_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this **non-convex** function, gradient descent might be unable to converge to one of the **global minima** and is stuck into one of the **local minima**. Of course, you could play with the learning rate back and forth until you manage to get to one of those global optima. But the machine hasn't the same capabilities as you (yet). All this to explain a simple fact: **there is no guarantees about reaching the global minima**. Fortunately, linear regression uses a convex cost function. :)\n",
    "\n",
    "Now, let's get back to simple linear regression. When applying gradient descent to a model, we need to use the data points of a dataset to adjust the parameters of the model in order to minimize a **cost function**. We already know what the cost function (see the $J$ function). In order to do that, there are two main flavors of gradient descent: (1) stochastic gradient descent and (2) batch gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "In SGD, we update the function parameters for each dataset observation we have. The generic pseudo-code for SGD is the following:\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. _$X'$ = shuffle($X$)_\n",
    "    2. _For each $x_n$ in $X'$_:\n",
    "        1. $\\omega = \\omega - \\alpha \\frac{\\partial f(x_n, \\omega)}{\\partial \\omega}$\n",
    "        \n",
    "where $\\omega$ is the set of parameters of your function $f$, $X$ is your dataset (input and targets included) and _shuffle_ is a function that returns a shuffled version of $X$.\n",
    "\n",
    "Let's transform the generic version into the simple linear regression optimization procedure by changing the partial derivatives definition. In order to do that, we need to get the partial derivatives of $J$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_0} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_1} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_1} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) x_n $$\n",
    "\n",
    "Now, let's adapt the pseudo code\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. _$X'$ = shuffle($X$)_\n",
    "    2. _For each $x_n$ in $X'$_:\n",
    "        1. $\\hat{y} = \\beta_0 + \\beta_1 x_n$\n",
    "        2. $b_0 = b_0 - \\alpha \\frac{\\partial J}{\\partial b_0} = b_0 + 2 \\alpha (y_n - \\hat{y}_n)$\n",
    "        3. $b_1 = b_1 - \\alpha \\frac{\\partial J}{\\partial b_1} = b_1 + 2 \\alpha (y_n - \\hat{y}_n)x_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def sgd_for_simple_linear_regression(x, y, b0, b1, learning_rate, epochs, random_state):\n",
    "    # Getting the numpy arrays of both x and y \n",
    "    # in order to make the calculations simple.\n",
    "    x = x.values\n",
    "    y = y.values\n",
    "    \n",
    "    # Initialize our random numbers generator \n",
    "    # with check_random_state.\n",
    "    random_state = check_random_state(random_state)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        # Shuffle our dataset.\n",
    "        x_, y_ = shuffle(x, y, random_state=random_state)\n",
    "        \n",
    "        for n in range(x.shape[0]): \n",
    "            # Extract a single x and y.\n",
    "            x__ = x_[[n]]\n",
    "            y__ = y_[[n]]\n",
    "            \n",
    "            # Compute the predictions.\n",
    "            y_hat = linear_regression(x__, b0, b1)\n",
    "            \n",
    "            # Compute the partial derivatives.\n",
    "            dJ_db0 = -2 * (y__ - y_hat)\n",
    "            dJ_db1 = -2 * (y__ - y_hat) * x__\n",
    "            \n",
    "            # Update the intercept and coefficient.\n",
    "            b0 = b0 - learning_rate * dJ_db0\n",
    "            b1 = b1 - learning_rate * dJ_db1\n",
    "            \n",
    "    return b0[0], b1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OFFTOPIC] You might be asking: *\"What the hell are random_state and check_random_state?\"*. For now, let's just say that *random_state* is the seed for a random numbers generator and *check_random_state* is a method that checks that seed and returns a random numbers generator.\n",
    "\n",
    "[BACK ON TOPIC] Now, back To make it clear how this works, let's use the following demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79309a7c69834ada850adb04a4cb0f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Floa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import sgd_simple_lr_dataset_demo\n",
    "\n",
    "sgd_simple_lr_dataset_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left side, you have the usual plot for simple linear regression. On the right, you have $\\beta_0$ and $\\beta_1$ plotted into a 2D plane, in order for you to see how the behavior changes when you increase or decrease the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo code for Multiple Linear Rregssion SGD is\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. _X' = shuffle(X)_\n",
    "    2. _For each $x_n$ in $X'$_:\n",
    "        1. $\\hat{y} = \\beta_0 + \\beta_1 x_n$\n",
    "        2. $b_0 = b_0 - \\alpha \\frac{\\partial J}{\\partial b_0} = b_0 + 2 \\alpha (y_n - \\hat{y}_n)$\n",
    "        3. *For k in 1...num_features:*\n",
    "            1. $b_k = b_k - \\alpha \\frac{\\partial J}{\\partial b_k} = b_k + 2 \\alpha (y_n - \\hat{y}_n)x_{k_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_multiple_linear_regression(x, y, betas, learning_rate, epochs, random_state):\n",
    "    # Initialize our random numbers generator \n",
    "    # with check_random_state.\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # Getting the numpy arrays of both x and y \n",
    "    # in order to make the calculations simple.\n",
    "    x = x.values\n",
    "    y = y.values\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        # Shuffle our dataset.\n",
    "        x_, y_ = shuffle(x, y, random_state=random_state)\n",
    "\n",
    "        for n in range(x.shape[0]): \n",
    "            # Extract a single x and y.\n",
    "            x__ = x_[[n]]\n",
    "            y__ = y_[[n]]\n",
    "\n",
    "            # Compute the predictions.\n",
    "            y_hat = multiple_linear_regression(x__, betas)\n",
    "\n",
    "            # Compute the partial derivatives.\n",
    "            dJ_db0 = -2 * (y__ - y_hat)\n",
    "            dJ_dbk = -2 * (y__ - y_hat) * x__.T\n",
    "\n",
    "            # Update the intercept and coefficient.\n",
    "            betas[0] = betas[0] - learning_rate * dJ_db0\n",
    "            betas[1:] = betas[1:] - learning_rate * dJ_dbk\n",
    "            \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "* For big datasets, if your model doesn't have _many_ parameters, the computational costs, both time and memory, will be very low.\n",
    "* It is able to escape from \"shallow\" local minima due to the randomness of the procedure and to a higher irregularity of the path token within the parameter space.\n",
    "\n",
    "**Cons**\n",
    "* If you have a smooth error curve/surface and not a big dataset, no need to go with this.\n",
    "* For smooth error curve/surfaces, if the dataset is not big, it can take more iterations to converge to the minimum than batch gradient descent would take.\n",
    "* You **really** need to keep an eye on the learning rate. Having a bigger learning rate can take you out of local minima but it can also cause overshooting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
