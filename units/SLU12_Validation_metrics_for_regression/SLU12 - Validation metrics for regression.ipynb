{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU12 - Validation metrics for regression: Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn about:\n",
    "* [Mean Absolute Error (MAE)]()\n",
    "* [Mean Squared Error (MSE)]()\n",
    "* [Root Mean Squared Error (RMSE)]()\n",
    "* [Coefficient of Determination (R²)]()\n",
    "* [Adjusted R²]()\n",
    "* [Regularization]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SLU9, you were introduced to one of the most intuitive and used regression models. You were also introduced to function ($J$) that measured how good the linear regression model was. In this SLU, we will take a look at that function, and others, more in-depth. \n",
    "\n",
    "But, before we start, there is something we should make it clear first. It will be very usual, while studying & practising data science, that you will hear/read these two words: **loss** and **metric**. Both of the refer to functions that evaluate the **quality** of a model. Sometimes, people will use both as they are the same thing. But really important differences between them:\n",
    "* **Loss** is the function that your model will minimize;\n",
    "* **Metric** is the function that you really want to use to evaluate how good your model is.\n",
    "\n",
    "Sometimes, the loss and metric functions are the same (example: linear regression). But other times, e.g. in classification, the metric will be really different from the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAE = \\frac{1}{N} \\sum_{n=1}^N \\left|y_n - \\hat{y}_n\\right|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = lambda y, y_hat: np.abs(y - y_hat).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The lower, the better.\n",
    "* It is less sensible to outliers than MSE (the next metric).\n",
    "* The output can be interpreted as the expected error measured in the same units as the target.\n",
    "* It can be used as both a metric and a loss function. There are some important caveats to take into consideration when doing using MAE as a loss function: (a) the number of solutions, (b) large jumps in the values of the parameters and (c) not having a derivative when MAE is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SLU9, we already explored this metric\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda y, y_hat: ((y - y_hat)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The lower, the better.\n",
    "* MSE can be used as both a metric and a loss function (e.g. linear regression).\n",
    "* It is sensible to outliers in its' original form.\n",
    "* The units of the metric are not the same as the ones used in the target. For example, if you are predicting house prices, i.e. the output is \\$, then the output would have units like \\$²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RMSE = \\sqrt{MSE}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda y, y_hat: np.sqrt(mse(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The lower, the better.\n",
    "* Its' output can be interpreted as having the same units as the targets.\n",
    "* MSE can be used as both a metric and a loss function. If fact, if we computed its partial derivative is $\\frac{\\partial RMSE}{\\partial \\hat{y}} = \\frac{1}{2 \\sqrt{MSE}} MSE$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination (R²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R² compares how better your regression model is when compared with a predictor that outputs just the mean of the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bar{y} = \\frac{1}{N} \\sum_{n=1}^N y_n$$\n",
    "\n",
    "$$R² = 1 - \\frac{MSE(y, \\hat{y})}{MSE(y, \\bar{y})} \n",
    "= 1 - \\frac{\\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)}{\\frac{1}{N} \\sum_{n=1}^N (y_n - \\bar{y})}\n",
    "= 1 - \\frac{\\sum_{n=1}^N (y_n - \\hat{y}_n)}{\\sum_{n=1}^N (y_n - \\bar{y})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = lambda y, y_hat: 1 - (mse(y, y_hat) / mse(y, np.mean(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the R², the more sure you are that the independent variables you used explain how the dependent variable changes. For example, if you got a R² of 0.7, you can say that the set of features you used are able to explain 70% of the target variable. (TODO: verificar se este parágrafo está certo).\n",
    "\n",
    "The higher R² you can get is 1. If you get R² = 0, it means that your model doesn't explain anything in the target by using the features you selected. If you get R² < 0, you are probably suffering too much with overshooting (remember the SGDRegressor example in SLU9?). Also, another reason for having R² < 0 is that the model you used doesn't make sense for that data you have.\n",
    "\n",
    "Also, when using R², there are something important [caveats](https://en.wikipedia.org/wiki/Coefficient_of_determination#Caveats) to take into account. One of the caveats is that, depending on the model, using more features can inflate the R² when, in fact, those features are really noisy, meaning the model is actually fitting to the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted R²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to take into account the addition os useless variables, we can use the adjusted R² score\n",
    "\n",
    "$$R_{adj}^2 = 1 - \\frac{N - 1}{N - K - 1} (1 - R^2)$$\n",
    "\n",
    "where $N$ is the number of observations in the training dataset and K is the number of features your model is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r2 = lambda y, y_hat, N, K: 1 - ((N - 1) / (N - K - 1)) * (1 - r2(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
