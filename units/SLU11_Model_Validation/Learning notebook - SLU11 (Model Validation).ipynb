{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU11: Model Validation\n",
    "---\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "* Bias-variance tradeoff\n",
    "* Generalization error\n",
    "* Underfitting and overfitting\n",
    "* Train and test errors\n",
    "* Train-test split (aka holdout method)\n",
    "* Train-validation-test split\n",
    "* Cross-validation\n",
    "* Bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "One way to understand overfitting is by decomposing the generalization error of a model into bias and variance.\n",
    "\n",
    "We will be using data about craft beer to try to predict whether a particular beer is an [India Pale Ale (IPA)](https://en.wikipedia.org/wiki/India_pale_ale).\n",
    "\n",
    "The data was preprocessed in advance, as the original dataset was simplified and manipulated for teaching purposes. There are two features:\n",
    "* `IBU`, which stands for International Bitterness Units and is a measure of bitterness\n",
    "* `Color`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data/beer.csv')\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bias-variance tradeoff\n",
    "\n",
    "## 2.1 The critical role of assumptions\n",
    "\n",
    "In machine learning, unlike other optimization problems, the function *f* we want to optimize is unknown to us - we have to *learn* it.\n",
    "\n",
    "All we have are inputs *X* and outputs *y*, and a big black box in the middle:\n",
    "\n",
    "![black_box](assets/black_box.png)\n",
    "\n",
    "*Fig 1: In machine learning, we come up a hypothesis and fine tune it using known X and y values to build an accurate model*\n",
    "\n",
    "We use (in fact, we *choose*) a hypothesis *h* to approximate the target function *f* (that maps the input X to the output *y*), given a set of parameters:\n",
    "\n",
    "$$ \\hat{y} = h_\\Theta(X) $$\n",
    "\n",
    "The hypothesis makes the target function *f* learnable by making assumptions about its *form* to simplify reality, such as this (well-known) one:\n",
    "\n",
    "$$ h_\\Theta(X) = \\theta_0 + \\theta_1x_1 + \\space ... \\space + \\theta_ix_i $$\n",
    "\n",
    "Parameters give our hypothesis flexibility and adaptability, but less underlying structure. They are *trained* to minimize how wrong the model is, given the data:\n",
    "\n",
    "$$ min \\space J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^m \\mathcal{L}(h_\\Theta(X^{(i)}), y^{(i)}) $$\n",
    "\n",
    "Training is limited by the underlying assumptions: there is no single model that works best for every problem, regardless of the amount of data.\n",
    "\n",
    "Let's start by isolating our features *X* and our target variable *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "y = X.pop('IsIPA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The need for validation\n",
    "\n",
    "Given the above, we need to validate our models after training, to know if they are any good:\n",
    "1. Our assumptions may not hold (that is, we trained a garbage model) or there may be better models\n",
    "2. We may be learning parameters that don't generalize for the entire population (i.e., statistical noise).\n",
    "\n",
    "Remember, our goal is to approximate the true, universal target function *f* and we need our model to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Decomposing the generalization error\n",
    "\n",
    "Bias-variance decomposition is a way of analyzing an algorithm's expected generalization error, with respect to the sum of three terms:\n",
    "\n",
    "1. Bias\n",
    "2. Variance\n",
    "3. Irreducible error.\n",
    "\n",
    "As we will see, dealing with bias and variance is really about under- (high bias) and over-fitting (high variance).\n",
    "\n",
    "![dart_throwing_bias_var_tradeoff](assets/dart_throwing_bias_var_tradeoff.png)\n",
    "\n",
    "*Fig. 2: Graphical illustration of bias and variance using dart-throwing, from [Scott Fortmann-Row's \"Understanding the Bias-Variance Trade-off\"](http://scott.fortmann-roe.com/docs/BiasVariance.html)*\n",
    "\n",
    "### 2.2.1 Bias and underfitting\n",
    "\n",
    "Bias results from simplistic assumptions and a lack of malleability: in short, we are missing parameters that would be in a correct model.\n",
    "\n",
    "Bias is always learning the same wrong thing, skewing predictions consistently across different training samples (i.e., far-off from the real value):\n",
    "\n",
    "$$ Bias = E\\big[\\hat{y} - y\\big] $$\n",
    "\n",
    "Fixing bias requires adding complexity to our models to make them more adaptable to the data. \n",
    "\n",
    "### 2.2.2 Variance and overfitting\n",
    "\n",
    "On the other side, extremely flexible models overreact to the specifics of the training data (including the random noise).\n",
    "\n",
    "Variance creeps in when we have more parameters than justified by the data and learn random things from different training samples:\n",
    "\n",
    "$$ Variance = E\\big[\\big(\\hat{y} - E[\\hat{y}]\\big)^2\\big] $$\n",
    "\n",
    "Fixing variance requires decreasing complexity to prevent the model from drifting.\n",
    " \n",
    "### 2.2.3 The bias-variance tradeoff explained\n",
    "\n",
    "There would be no tradeoff if we knew the perfect hypothesis in advance and had unlimited data to learn the parameters from.\n",
    "\n",
    "In reality, there is an explicit trade-off between the two and bias is reduced and variance is increased in relation to model complexity.\n",
    "\n",
    "![bias_variance_trade_off](assets/bias_variance_trade_off.png)\n",
    "\n",
    "*Fig. 3: The bias-variance tradeoff, bias is reduced and variance is increased in relation to model complexity*\n",
    "\n",
    "By complexity we mean, among others, adding new features, increasing the polynomial degree of the hypothesis or using highly flexible models.\n",
    "\n",
    "In theory, we reach the right level of complexity when the increase in bias is equivalent to the reduction in variance:\n",
    "\n",
    "$$ \\frac{dBias}{dComplexity} = - \\frac{dVariance}{dComplexity} $$\n",
    "\n",
    "In practice, *there is not an analytical way to find this location* and the more we (over)reach for signal, the greater the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Code, at last!\n",
    "\n",
    "Below are different classifiers that perform differently in terms of bias and variance:\n",
    "1. Logistic Regression\n",
    "2. k-Nearest Neighbors with k=1\n",
    "3. k-Nearest Neighbors with k=0\n",
    "\n",
    "**Logistic Regression (high-bias)**\n",
    "\n",
    "Logistic regression provides an example of bias because it makes a lot of assumptions about the form of the target function.\n",
    "\n",
    "Visually, we can understand the model's inability to adjust to a non-linear decision boundary, structurally enforcing a linear one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_decision_regions(X.values, y.values, clf=lr, legend=2)\n",
    "plt.title(\"Logistic Regression (LR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors with k=1 (high-variance)**\n",
    "\n",
    "On the other side, the k-Nearest neighbors algorithm provides great flexibility and minimum underlying structure.\n",
    "\n",
    "The small blue *pockets* or *islands* show that our model is overadapting to the training data and, most probably, fitting to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "   \n",
    "knn_k1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_k1.fit(X, y)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_decision_regions(X.values, y.values, clf=knn_k1, legend=2)\n",
    "plt.title(\"KNN (k=1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors with k=9 (sort of better)**\n",
    "\n",
    "A key part of the k-NN algorithm is the choice of *k*: the number of nearest numbers in which to base the prediction.\n",
    "\n",
    "Increasing *k* results in considering more observations in each prediction and makes the model more rigid, for good effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_k9 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_k9.fit(X, y)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_decision_regions(X.values, y.values, clf=knn_k9, legend=2)\n",
    "plt.title(\"KNN (k=9)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing different models by plotting decision boundaries, however, is not very scientific, especially at higher dimensions.\n",
    "\n",
    "There must be a better way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Performing model validation\n",
    "\n",
    "The first thing we will do is creating a dictionary with our classifiers, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'LR': LogisticRegression(),\n",
    "        'KNN k=9': KNeighborsClassifier(n_neighbors=9),\n",
    "        'KNN k=1': KNeighborsClassifier(n_neighbors=1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 In-sample-error (ISE) or training error\n",
    "\n",
    "The in-sample-error is how well our model performs on the training data.\n",
    "\n",
    "We will measure the error rate for each model in the simplest way, by computing the fractions of misclassified cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def classification_error(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    error_rate = 1 - accuracy_score(y, y_pred)\n",
    "    return round(error_rate * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our model's performance on the training data is a common mistake and underestimates the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, clf in clfs.items():\n",
    "    clf.fit(X, y)\n",
    "    training_error = classification_error(clf, X, y)\n",
    "    print('---\\n{} error:\\nTrain: {}%'.format(key, training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Out-of-sample error (OSE) or testing error\n",
    "\n",
    "The out-of-sample error measures how well the model performs on previsouly unseen data and if it's picking up patterns that generalize well.\n",
    "\n",
    "Ideally, both training and test errors are low and close to one another.\n",
    "\n",
    "*Underfitted* models tend to perform poorly on both train and test data, having large (and similar) in-sample- and out-of-sample errors.\n",
    "\n",
    "*Overfitting* is detected when a model that performs on training data but not quite so well in the test set: the bigger the gap, the greater the overfitting.\n",
    "\n",
    "![train_test_error](assets/train_test_error.png)\n",
    "\n",
    "*Fig 4: How training and test errors behave in regards to model complexity, bias and variance*\n",
    "\n",
    "There are different techniques to measure the testing error, we will focus on:\n",
    "1. Train-test split\n",
    "2. Validation set\n",
    "3. Cross-validation\n",
    "4. Bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Train-test split (aka holdout method)\n",
    "\n",
    "The best possible solution is to leave a random subset of the data aside from the beginning to test our final model at the end.\n",
    "\n",
    "![test_set](assets/test_set.png)\n",
    "\n",
    "*Fig 5: Test set illustrated, you holdout a significant chunk of the data for testing your model in the end*\n",
    "\n",
    "We must keep the test data aside for the entire modelling process, otherwise knowledge about the test set will *leak* into the model and ruin the experiment.\n",
    "\n",
    "![train_test_set](assets/train_test_split.png)\n",
    "\n",
    "*Fig 5: Workflow with test and training sets*\n",
    "\n",
    "After evaluation, relearn your final model on the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "print(\"---\\nNumber of observations:\\nTrain: {} | Test: {}\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X_train, y_train, X_test, y_test, clf):\n",
    "    training_error = classification_error(clf, X_train, y_train)\n",
    "    test_error = classification_error(clf, X_test, y_test)\n",
    "    return training_error, test_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    print('---\\n{} error:\\nTrain: {:5}% | Test: {}%'.format(key, training_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Validation set\n",
    "\n",
    "Given we have enough data, we can create a *validation dataset*.\n",
    "\n",
    "![validation_set](assets/validation_set.png)\n",
    "\n",
    "*Fig. 6: Validation set as compared with the holdout approach*\n",
    "\n",
    "![validation_split](assets/validation_split.png)\n",
    "\n",
    "*Fig. 7: Workflow with test, validation and training sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25)\n",
    "del X_temp, y_temp\n",
    "\n",
    "print(\"Number of observations:\\nTrain: {} | Test: {} | Validation: {}\".format(X_train.shape[0], X_test.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(X_train, y_train, X_test, y_test, X_validation, y_validation, clf):\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    validation_error = classification_error(clf, X_validation, y_validation)\n",
    "    return training_error, test_error, validation_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error, validation_error = compute_validation_metrics(X_train, y_train, X_test, y_test, X_val, y_val, clf)\n",
    "    print('---\\n{} error:\\nTrain: {:5}% | Validation: {:4}% | Test: {:4}%'.format(key, training_error, validation_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 k-Fold Cross-validation\n",
    "\n",
    "Test error results can be subject to great variability, especially for smaller datasets, depending on how we split the data (i.e., which observations go where).\n",
    "\n",
    "Also, and quite obviously, holding out *more* data reduces the amount available for training, possibly leading us to *overestimate* the test error.\n",
    "\n",
    "In *k*-fold cross validation:\n",
    "1. The original sample is randomly partitioned into *k* equal sized parts, or subsamples\n",
    "2. Each time, we leave out part *k*, fit the model to the other *k*-1 subsets combined in a single dataset, and then test the model against the left out *k*th part\n",
    "3. This is done for each *k* = 1, 2, ..., *K*, and then the results are combined, using, for example, the mean error.\n",
    "\n",
    "![cross_validation](assets/cross_validation.png)\n",
    "\n",
    "*Fig 8: Creating multiple (K=5) train and test set pairs using cross-validation*\n",
    "\n",
    "This way, we use every observation to both train and test out model: each fold is used once as validation, while the *k*-1 remaining folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring=classification_error)\n",
    "    mean_error = round(np.mean(scores), 2)\n",
    "    var_error = round(np.var(scores), 2)\n",
    "    print('---\\n{} validation error:\\nMean: {:5}% | Variance: {:4}'.format(key, mean_error, var_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, since each training set contains part of the data, the estimated test error can, still, be biased upward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Bootstrapping\n",
    "\n",
    "Bootstraping consists of repeatedly sampling observations from the original data, with replacement, to create multiple bootstrap datasets with the same size *n* as the original dataset.\n",
    "\n",
    "We will use `np.random.choice()` to generate a random sample, with replacement, of * n* rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([1, 2, 3, 4, 5], size=5, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bootstrap_dataset(X, y):\n",
    "    n = X.shape[0]\n",
    "    train_obs = np.random.choice(X.index, size=n, replace=True)\n",
    "    return X.loc[train_obs], y.loc[train_obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will to generate many bootstrap datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y, n_datasets=1000):\n",
    "    datasets = {}\n",
    "    for i in range(0, n_datasets):\n",
    "        X_z, y_z = create_bootstrap_dataset(X, y)\n",
    "        dataset = {'X': X_z, 'y': y_z}\n",
    "        z = len(datasets)\n",
    "        datasets.update({z: dataset})\n",
    "    return datasets\n",
    "        \n",
    "datasets = bootstrap(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our model using our bootstrap datasets, and test it against the original dataset.\n",
    "\n",
    "For relevance, we are exluding those observations in the training set from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, clf in clfs.items():\n",
    "    results[name] = {}\n",
    "    for key, dataset in datasets.items():\n",
    "        X_train, y_train = dataset['X'], dataset['y']\n",
    "        X_test, y_test = X.drop(X_train.index), y.drop(y_train.index)\n",
    "        clf.fit(X_train, y_train)\n",
    "        training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "        results[name].update({key: test_error})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a distribution of the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for key, result in results.items():\n",
    "    prediction_errors = list(result.values())\n",
    "    mean_error = round(np.mean(prediction_errors), 2)\n",
    "    var_error = round(np.var(prediction_errors), 2)\n",
    "    sns.distplot(prediction_errors, label=key)\n",
    "    print('---\\n{} validation error:\\nMean: {:5}% | Variance: {:4}'.format(key, mean_error, var_error))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
