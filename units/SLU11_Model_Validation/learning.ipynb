{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU11: Model Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 About the data\n",
    "\n",
    "We will be using data about craft beer to try to predict whether a particular beer is an Indian Pale Ale (IPA), based on its bitterness and color. From the [Wikipedia](https://en.wikipedia.org/wiki/India_pale_ale):\n",
    "\n",
    "> India Pale Ale (IPA) is a hoppy beer style within the broader category of pale ale. (...) The term *pale ale* originally denoted an ale that had been brewed from pale malt.\n",
    "\n",
    "The data was preprocessed in advance, as the original dataset was simplified and manipulated for teaching purposes. There are two features:\n",
    "* `IBU`, which stands for International Bitterness Units and is a measure of bitterness\n",
    "* `Color`.\n",
    "\n",
    "The response variable `IsIPA` is binary and indicates whether or not a given observation is an IPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/beer.csv')\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "y = X.pop('IsIPA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 Overfitting\n",
    "<a id=\"overfitting\"></a>\n",
    "\n",
    "One way to understand overfitting is by decomposing the generalization error of a model into bias and variance.\n",
    "\n",
    "### 2.1 Bias-variance trade-off\n",
    "\n",
    "#### 2.1.1 Generalization error\n",
    "\n",
    "In machine learning, unlike most other optimization problems, we don't know the function we want to optimize.\n",
    "\n",
    "Our goal is to estimate the target function *f* that maps input data X to the output variable *y*, given our hypothesis and some parameters (i.e., the knobs):\n",
    "\n",
    "$$ \\hat{y} = h_\\Theta(X) $$\n",
    "\n",
    "To do so, we minimize how wrong the model is by comparing estimated predictions against real values:\n",
    "\n",
    "$$min \\space J(\\Theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance decomposition is a way of analyzing an algorithm's expected generalization error, with respect to the sum of three terms:\n",
    "\n",
    "1. Bias\n",
    "2. Variance\n",
    "3. Irreducible error.\n",
    "\n",
    "As we will see, dealing with bias and variance is really about under- (high bias) and over-fitting (high variance).\n",
    "\n",
    "#### 2.1.3 Error due to bias and *underfitting*\n",
    "\n",
    "Bias typically results from oversimplifying assumptions, skewing results consistently across all realizations of the model and lacking the flexibility to fit underlying patterns. \n",
    "\n",
    "It measures how far off these predictions are from the actual value by taking the difference between the expected prediction and the correct value:\n",
    "\n",
    "$$ Bias = (E[\\hat{y}] - y)^2 $$\n",
    "\n",
    "Bias is the tendency to learn the same wrong thing, and represents *underfitting* (i.e., missing parameters that would appear in a correctly specified model).\n",
    "\n",
    "Fixing bias requires adding complexity to our models.\n",
    "\n",
    "#### 2.1.4 Error due to variance and *overfitting*\n",
    "\n",
    "Highly flexible and data-adaptive approaches, on the other side, may lead the model to overreact to the specifics of the training data and, thus, to high variance.\n",
    "\n",
    "Along the same lines, the variance is how much the predictions for a given point vary betweeen the different realizations of the model:\n",
    "\n",
    "$$ Variance = E[(\\hat{y} - y)^2] $$\n",
    "\n",
    "Variance is the tendency to learn random things, and is a sign of *overfitting* (i.e., including more parameters than can be justified by the data).\n",
    "\n",
    "Fixing variance requires decreasing model complexity.\n",
    "\n",
    "#### 2.1.5 The trade-off\n",
    "\n",
    "![dart_throwing_bias_var_tradeoff](assets/dart_throwing_bias_var_tradeoff.png)\n",
    "\n",
    "*Fig. 1: Graphical illustration of the bias-variance trade-off, borrowed from [Scott Fortmann-Row's \"Understanding the Bias-Variance Trade-off](http://scott.fortmann-roe.com/docs/BiasVariance.html)*\n",
    "\n",
    "The need for generalization has a major consequence: data alone is not enough, regardless of how much of it we have. \n",
    "\n",
    "In the real world of imperfect models and limited data, there is an explicit trade-off: \n",
    "\n",
    "> Bias is reduced and variance is increased in relation to model complexity (e.g., additional features, increasing polynomial, using highly flexible models)\n",
    "\n",
    "![bias_variance_tradeoff_complexity](assets/right_fited_model.png)\n",
    "\n",
    "*Fig. 2: How bias and variance relate to model complexity, an adaptation from Scott Fortman-Row's article [found here](http://www.ebc.cat/2017/02/12/bias-and-variance/)*\n",
    "\n",
    "In theory, we reach the right level of complexity when the increase in bias is equivalent to the reduction in variance:\n",
    "\n",
    "$$ \\frac{dBias}{dComplexity} = - \\frac{dVariance}{dComplexity} $$\n",
    "\n",
    "In practice, *there is not an analytical way to find this location*. Below are three different models that perform differently in terms of bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "clfs = {'Logistic regression': LogisticRegression(),\n",
    "        'KNN (N=9)': KNeighborsClassifier(n_neighbors=9),\n",
    "        'KNN (N=1)': KNeighborsClassifier(n_neighbors=1)}\n",
    "   \n",
    "for key, clf in clfs.items():\n",
    "    plt.figure()\n",
    "    plt.title(key)\n",
    "    plot_decision_regions(X.values, y.values, clf=clf.fit(X, y), legend=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Evaluating model performance\n",
    "\n",
    "### 3.1 In-sample-error (ISE) or training error\n",
    "\n",
    "\n",
    "The in-sample-error is how well our model performs on the training data, regardless of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def classification_error(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    error_rate = 1 - accuracy_score(y, y_pred)\n",
    "    return round(error_rate * 100, 2)\n",
    "                 \n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X, y)\n",
    "    training_error = classification_error(clf, X, y)\n",
    "    print('{} error:\\nTrain: {}%'.format(key, training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This alone is wrong because we want our models to learn, rather than memorize (e.g., like a lookup table). Remember: the purpose of a model is to generalize to unseen data.\n",
    "\n",
    "Testing our model on the training data is a common mistake and, although it can be a useful metric, it tells part of the story.\n",
    "\n",
    "### 3.2 Out-of-sample error (OSE) or testing error\n",
    "\n",
    "The out-of-sample error is, precisely, how well the model previsouly on new data. It's the second part of the story.\n",
    "\n",
    "What we want to understand here is if the model iis picking up patterns that generalize.\n",
    "\n",
    "![different_validation_methods](assets/different_validation_techniques.png)\n",
    "\n",
    "*Fig. 3:* Different validation techniques, as illustrated by [Towards Data Science's \"Train/Test Split and Cross Validation in Python\"](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "\n",
    "#### 3.1.1 Train-test split (aka holdout method)\n",
    "\n",
    "The most common way of measuring the OSE is setting some of the data aside from the beginning and only use it to test your classifier at the end, when modeling is complete.\n",
    "\n",
    "![test_set](assets/test_set.png)\n",
    "\n",
    "*Fig 4: Test set illustrated*\n",
    "\n",
    "Please, keep your test data aside for the entire modelling process, otherwise you're using it to train your model.\n",
    "\n",
    "If you are confident about your final classifier after evaluation, you should relearn it on the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "print(\"Number of observations:\\nTrain: {} | Test: {}\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X_train, y_train, X_test, y_test, clf):\n",
    "    training_error = classification_error(clf, X_train, y_train)\n",
    "    test_error = classification_error(clf, X_test, y_test)\n",
    "    return training_error, test_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    print('{} error:\\nTrain: {:5}% | Test: {}%'.format(key, training_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, both training and test errors are low and close to one another. The model has anough flexibility to pick up the signal (some of it, at least) and not the noise.\n",
    "\n",
    "Of course, holding out data reduces the amount available for training. Also, for small datasets, the results can be subject to great variability, depending on how you split the data.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "Overfitting is detected when a model that performs on training data but not quite so well in the test set: the bigger the gap, the greater the overfitting.\n",
    "\n",
    "**Underfitting**\n",
    "\n",
    "On the other side, underfitted models tend to perform poorly on both train and test data, having large (and similar) in-sample- and out-of-sample errors.\n",
    "\n",
    "#### 3.1.2 Validation set\n",
    "\n",
    "If you need to keep the test data aside until we have a final model, how can we diagnose under- and overfitting? Given we have enough data, we create a validation dataset.\n",
    "\n",
    "![validation_set](assets/validation_set.png)\n",
    "\n",
    "*Fig. 5: Validation set as compared with the holdout approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25)\n",
    "del X_temp, y_temp\n",
    "\n",
    "print(\"Number of observations:\\nTrain: {} | Test: {} | Validation: {}\".format(X_train.shape[0], X_test.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(X_train, y_train, X_test, y_test, X_validation, y_validation, clf):\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    validation_error = classification_error(clf, X_validation, y_validation)\n",
    "    return training_error, test_error, validation_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error, validation_error = compute_validation_metrics(X_train, y_train, X_test, y_test, X_val, y_val, clf)\n",
    "    print('---\\n{} error:\\nTrain: {:5}% | Test: {:4}% | Validation: {:4}%'.format(key, training_error, test_error, validation_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 *K*-fold cross validation\n",
    "\n",
    "In *k*-fold cross validation (where *K* represents the number of train/validation splits) the original sample is randomly partitioned into *k* equal sized subsamples.\n",
    "\n",
    "![cross_validation](assets/cross_validation.png)\n",
    "\n",
    "This method is most suited for small to medium size datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring=classification_error)\n",
    "    mean_error = round(np.mean(scores), 2)\n",
    "    var_error = round(np.var(scores), 2)\n",
    "    print('---\\n{} validation error:\\nMean: {:5}% | Variance: {:4}'.format(key, mean_error, var_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
