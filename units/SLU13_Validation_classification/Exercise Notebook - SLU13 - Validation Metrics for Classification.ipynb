{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a239644097794a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercises Notebook - SLU13 - Validation Metrics for Classification\n",
    "Associated presentation [here](https://docs.google.com/presentation/d/1lEE9BUWsUKryXzGCLyysX7d78XL3ylANTU-fMKtIeYE/edit?usp=sharing). This notebook only covers validation metrics for **binary classification**.\n",
    "\n",
    "----\n",
    "*By: [Hugo Lopes](https://www.linkedin.com/in/hugodlopes/)  \n",
    "LDSA - SLU13*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80b6ded3c7e4ff71",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50a2e3f5c827c482",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "    \n",
    "from utils import plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f767181ff8a74302",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 1. Load Data!\n",
    "\n",
    "First, let us load some data to fit into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-183ae06e20a51d99",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN this cell\n",
    "df = pd.read_csv('exercise_dataset_SLU13.csv')\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09fdc6498dfbee6a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As we can see, we have our target named `label` and then 10 columns, that are our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-309aeaa50a83f366",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Dataset Imbalance\n",
    "Some performance metrics are not recommended its use in highly imbalanced datasets. Check the imbalance of your dataset. What can you tell about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0eda277e12939a26",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def class_imbalance(labels):\n",
    "    \"\"\"\n",
    "    Calculate the class imbalance\n",
    "    \"\"\"\n",
    "    # Calculate the class imbalance, i.e., the ratio of 1s (ones)\n",
    "    # in the dataset\n",
    "    # ratio_1s = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    ratio_1s = labels.mean()\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return ratio_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94bd00c67a1fe4c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Ratio of 1s (imbalance):', class_imbalance(df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6079d75d48915987",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Ratio of 1s (imbalance): 0.0702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-030786b1768fc408",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert np.isclose(class_imbalance(df['label']), 0.0702, atol=1e-3)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ac5292b4570c33a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So, this result should put us on alert for the evaluation metrics already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e3cc9c61f3335b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Divide into Train and Test sets\n",
    "Remember: always keep a part of your data separate for final evaluation of its performance. Time to do that:\n",
    "- X_train: train data  \n",
    "- y_train: target of train data  \n",
    "- X_test: test data  \n",
    "- y_test: target of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee81d9fbdee5c5a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN this cell\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), \n",
    "                                                    df['label'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-959a32d33b84d1fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Fit the Logistic Regression with Train Set\n",
    "Let's fit the Logistic Regression on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13180965a7f1de43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN cell:\n",
    "clf = LogisticRegression(random_state=123, tol=1e-8).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7b9cadddaa7d3c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Getting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-280d064d01d162c9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_probas(clf, X_test):\n",
    "    \"\"\"\n",
    "    Get the predictions (probas) for a test set 'X_test' with a fitted classifier 'clf'\n",
    "    \n",
    "    Inputs:\n",
    "        clf: Logistic Regression classifier (sklearn classifier)\n",
    "        X_test: test dataset (pandas.DataFrame) (Num_rows, num_features)\n",
    "    \n",
    "    Output:\n",
    "        probas: predicted probabilities (numpy.array, of shape (Num_rows,))\n",
    "    \"\"\"\n",
    "    # Get predictions on the test set, i.e., get the _probabilities of being of class 1_ \n",
    "    # for the Test set (`X_test`) by making use of the method \n",
    "    # `predict_proba` of your classifier. Assign it to the variable `probas`\n",
    "    # NOTE: don't forget to extract only the second column.\n",
    "    # probas = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    probas = clf.predict_proba(X_test)[:, 1]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f6777972243da4f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "probas = calc_probas(clf, X_test)\n",
    "print('Probabilities:', probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4828a09c677f7eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Probabilities: [0.07192596 0.06016595 0.06723508 ... 0.08183457 0.05332582 0.0543554 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e906e89e01c760bb",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert np.isclose(probas[0], 0.07192596, atol=1e-5)\n",
    "assert len(probas) == 3300, \"The length of the variable 'probas' is expected to be 3300.\"\n",
    "assert type(probas) == np.ndarray\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dfb578b62308daa5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Binarize Predictions and Confusion matrix\n",
    "You should have by now the `probas` (you array of probabilities of being 1). There is a point in time where you will have to transform your predictions in the range [0, 1] (e.g., `[0.11582418 0.04812204]`) to something like 0 and 1, or Yellow and Blue. This means you will have _to take a decision_. This decision is taken by taking into account the business characteristics. \n",
    "\n",
    "For example, if you want to raise a warning if a person has cancer, you might not want to raise it only when you get a probability of 50% right? It has less consequences to have a False Positive than a False Negative in this case, and the person will thank you for that.\n",
    "\n",
    "This action of _taking a decision_ is generally done by setting a **threshold** on your predictions, where above that threshold you set all your prediction as `1` and below it you set them as `0`.\n",
    "\n",
    "Let's do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43efa02831475bb3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def binarize_probas(probas, threshold):\n",
    "    \"\"\"\n",
    "    Transform probas to 0 or 1 depending on the threshold.\n",
    "    \n",
    "    Inputs:\n",
    "        probas: predicted probabilities (numpy.array, of shape (N,))\n",
    "        threshold: threshold to convert probas in binary vector (float)\n",
    "    \n",
    "    Output:\n",
    "        predictions (numpy.array, of shape (N,)), dtype=int\n",
    "    \"\"\"\n",
    "    # Transform your float array of `probas` to an int array where\n",
    "    # the value 0 is below or equal to 'threshold' and 1 is above the \n",
    "    # 'threshold'\n",
    "    # predictions = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    predictions = (probas > threshold).astype(int)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23a5f5d590771c51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "my_threshold = 0.15\n",
    "\n",
    "predictions = binarize_probas(probas, my_threshold)\n",
    "print('Array of predictions:', predictions[-36:])\n",
    "print('Number of 1s (above threshold):', predictions.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53f58a856a25b889",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Array of predictions: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "    Number of 1s (above threshold): 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b0475bb6576f9a6f",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert predictions.sum() == 168\n",
    "assert len(predictions) == 3300, \"The length of the variable 'predictions' is expected to be 3300.\"\n",
    "assert predictions.dtype == np.int64\n",
    "assert predictions[-1] == 0\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4df6efa366e0554f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Get the TP, FP, TN, FN\n",
    "The TP, FP, TN and FN can be obtained by using the [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) of sklearn. Let's use it (check its documentation if you need to). We need these metrics to calculate the accuracy, precision and recall in the enxt exercise.\n",
    "\n",
    "**Important**: you must have completed the previous exercise correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49baf011180c6d08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_confmat(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the TP, FP, TN, FN using the sklearn confusion matrix.\n",
    "    \n",
    "    Inputs:\n",
    "        predictions: predictions (0 or 1) (numpy.ndarray)\n",
    "        y_true: true labels (0 or 1) (numpy.ndarray)\n",
    "        \n",
    "    Output:\n",
    "        Dictionary with TP, FP, TN and FN values\n",
    "    \"\"\"\n",
    "    # Get the TP, FP, TN, FN from `confusion_matrix(...)` of sklearn\n",
    "    # Assign to the following variables:\n",
    "    # tn, fp, fn, tp = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9271e7e8f9e640f8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "confmat = get_confmat(predictions, y_test.values)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28b321d6cd55918d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    {'TP': 64, 'FP': 104, 'TN': 2971, 'FN': 161}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7052c12dff7ba619",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert confmat['TP'] == 64\n",
    "assert confmat['FP'] == 104\n",
    "assert confmat['TN'] == 2971\n",
    "assert confmat['FN'] == 161\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a9296e5c35f2bc4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Calculating Accuracy, Precision and Recall by hand\n",
    "Best way to learn how things work is to do them by hand. Let's implement the following three simple metrics by hand: \n",
    "- The **Accuracy** is the fraction (default) or the count (normalize=False) of correct predictions. It is given by:  \n",
    "\n",
    "$$ A = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "\n",
    "- **Precision** is the ability of the classifier not to label as positive a sample that is negative (i.e., a measure of result relevancy).\n",
    "$$ P = \\frac{TP}{TP+FP} $$  \n",
    "  \n",
    "  \n",
    "- **Recall** is the ability of the classifier to find all the positive samples (i.e., a measure of how many truly relevant results are returned).\n",
    "$$ R = \\frac{TP}{TP+FN} $$  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19648a0d1cc1f1aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_metrics(confmat):\n",
    "    \"\"\"\n",
    "    Calculate Accuracy, Precision and Recall performance metrics.\n",
    "    DO NOT use sklearn - Implementation by hand.\n",
    "    \n",
    "    Inputs:\n",
    "        confmat: Dictionary with TP, FP, TN and FN values (dict)\n",
    "        \n",
    "    Output:\n",
    "        Dictionary with accuracy, precision and recall metrics\n",
    "    \"\"\"\n",
    "    # Extracting the needed metrics - to ease readability\n",
    "    tn = confmat['TN']\n",
    "    fp = confmat['FP']\n",
    "    fn = confmat['FN']\n",
    "    tp = confmat['TP']\n",
    "    \n",
    "    # Calculate Accuracy and assign it to the variable 'accuracy'\n",
    "    # accuracy = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # Calculate Precision and assign it to the variable 'precision'\n",
    "    # precision = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    precision = tp / (tp + fp)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # Calculate Recall and assign it to the variable 'recall'\n",
    "    # recall = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    recall = tp / (tp + fn)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b7e1d089248b805",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = calc_metrics(confmat)\n",
    "print('Accuracy: %.2f' % metrics['accuracy'])\n",
    "print('Precision: %.2f' % metrics['precision'])\n",
    "print('Recall: %.2f' % metrics['recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d537c3a5e8da719",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Accuracy: 0.92\n",
    "    Precision: 0.38\n",
    "    Recall: 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5ff24bc34c52f97",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert np.isclose(metrics['accuracy'], 0.9196969696969697, atol=1e-4)\n",
    "assert np.isclose(metrics['precision'], 0.38095238095238093, atol=1e-4)\n",
    "assert np.isclose(metrics['recall'], 0.28444444444444444, atol=1e-4)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b00319b2812cd3c7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6: Calculate AU ROC curve using Sklearn\n",
    "The Receiver Operating Characteristic (ROC) curve is a very common (and important) metric for **binary classification problems**. \n",
    "\n",
    "**Formally**, it is created by plotting the fraction of true positives out of the positives (TPR = true positive rate, a.k.a., sensitivity) vs. the fraction of false positives out of the negatives (FPR = false positive rate, or 1-specificity), at various threshold settings.  \n",
    "- The [**`roc_auc_score`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) function computes the Area Under the ROC curve (AUROC). I.e., the curve information is summarized in one number.  \n",
    "\n",
    "Let's check its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-676a58eb3ed5f6af",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_auc(probas, y_true):\n",
    "    \"\"\"\n",
    "    Get the AU ROC taking the inputs:\n",
    "    - probas: your predictions (e.g., probabilities)\n",
    "    - y_true: the actual outcomes (0 or 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the Area Under ROC Curve. Use the sklearn implementation\n",
    "    # 'roc_auc_score(...)'\n",
    "    # auc = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    auc = roc_auc_score(y_true, probas)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-314374b120cc13e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "auc = get_auc(probas, y_test)\n",
    "print('Area Under ROC curve: %.4f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8d053a5d581a92f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Area Under ROC curve: 0.6948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f998cca62cc9882d",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN TESTS\n",
    "assert np.isclose(auc, 0.6948, atol=1e-3)\n",
    "### END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73e9fab048f0a91a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Looks like the accuracy metric was somewhat misleading right? Our classifiers is not that good from the AUC point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f332356383a4893",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# [EXTRA]: Taking a look at the ROC curve\n",
    "Taking a visual look at the ROC curve is also important to diagnose model problems. For example, if you see the curve crossing the diagonal of the chart (random behaviour) you might have a problem. So, it is recommended to combine both summary metric AUROC and the data visualization.\n",
    "\n",
    "Let's take a look at it.\n",
    "You can use the [**`roc_curve`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) of sklearn to compute Receiver Operating Characteristic (ROC) curve points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0dc967f02469cfb4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the False Positive Rate and the True Positive Rate values\n",
    "fpr, tpr, _ = roc_curve(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5ac554cabad3210",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Call an handy plotting function (you can take a look at its code in the utils.py file)\n",
    "plot_roc_curve(auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6922e33796ef4ce1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Looking good, right?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
