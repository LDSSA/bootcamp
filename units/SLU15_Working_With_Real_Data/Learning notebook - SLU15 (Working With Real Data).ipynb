{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU15: Working With Real Data\n",
    "---\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "* Tidy data\n",
    "* Numerical data\n",
    "* Scaling\n",
    "* Ordinal data\n",
    "* Label encoding\n",
    "* Categorical data\n",
    "* Categorical dtype\n",
    "* Get dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Happy datasets are all alike; every unhappy dataset is unhappy in its own way.\n",
    "\n",
    "(Shamelessly adapted from [Tolstoy's Anna Karenina](https://en.wikipedia.org/wiki/Anna_Karenina_principle).)\n",
    "\n",
    "# 1 Tidy data principles\n",
    "\n",
    "At the beginning of any project, it is critical to structure datasets in a way that facilitates work.\n",
    "\n",
    "Most datasets are dataframes made up of rows and columns, containing values that belong to a variable and an observation:\n",
    "* *Variables* contain all values that measure the same thing across observations\n",
    "* *Observations* contain all values measured on the same unit (e.g., same person) across variables.\n",
    "\n",
    "The ideas of *tidy data* ([Wickham, 2014](http://vita.had.co.nz/papers/tidy-data.html)) provide a standardized framework to organize and structure datasets, making them easy to manipulate, model and visualize.\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table (or dataframe).\n",
    "\n",
    "We will be using a preprocessed version of the `avengers` dataset, by [FiveThirtyEight](http://fivethirtyeight.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "avengers = pd.read_csv('data/avengers.csv')\n",
    "avengers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Types of data in Pandas\n",
    "\n",
    "As stated above, a dataset is a collection of values, usually either numbers (quantitative) or strings (qualitative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas main data types are:\n",
    "* Numeric (`int`, `float`)\n",
    "* Datetime (`datetime`, `timedelta`)\n",
    "* Object (for strings).\n",
    "\n",
    "The convenient `DataFrame.select_dtypes` allows us to select variables (columns in our dataframe) by data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avengers.select_dtypes(include='object')\n",
    "         .head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Apply functions over variables (or columns)\n",
    "\n",
    "Pandas provides us with a convenient `df.apply` method that enables us to apply over entire columns. \n",
    "\n",
    "Let's use it to compute the mean and the mode for numeric and non-numeric values, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "avengers.apply(stats.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `df.select_dtypes` and `df.apply` together to compute the mean for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avengers.select_dtypes(include='int64')\n",
    "         .apply(np.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Apply functions over observations (or rows)\n",
    "\n",
    "Alternatively, we can use `df.apply` to apply functions over rows with a little adjustment, by setting `axis=1`.\n",
    "\n",
    "Let's use it to compute the norm of our row vectors (sort of, since we are considering only the numerical columns for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "(avengers.select_dtypes(include='int64')\n",
    "         .apply(norm, axis=1)\n",
    "         .sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an experiment and so you see two different use cases, let's try to scale each row to a unit vector:\n",
    "1. We will use `df.apply` to divide *each value or cell* by the norm of the row vector\n",
    "2. We will use `df.apply` to compute the norm of the *entire row*, just like we did above, to see if we succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(row):\n",
    "    \"\"\"\n",
    "    Takes a vector of values and transforms it into a unit vector with length 1.\n",
    "    This is achieved by computing v / ||v|| for each value in the row vector.\n",
    "    \"\"\"\n",
    "    return row / norm(row)\n",
    "\n",
    "(avengers.select_dtypes(include='int64')\n",
    "         .apply(normalize, axis=1)\n",
    "         .apply(norm, axis=1)\n",
    "         .sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Types of statistical data\n",
    "\n",
    "There are three main types of statistical data:\n",
    "1. Numerical\n",
    "2. Categorical\n",
    "3. Ordinal (which is a little bit of both, as you will see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Numerical data\n",
    "\n",
    "Numerical data is information that is measurable. It's always collected in number form, although not all data in number form is numerical.\n",
    "\n",
    "Things we can do with numerical data:\n",
    "* Mathematical operations (e.g., addition, distances and the normalization above)\n",
    "* Sort it in ascending or descending order.\n",
    "\n",
    "**Discrete data**\n",
    "\n",
    "Discrete data take on certain values, although the list of values may be finite or not. \n",
    "\n",
    "Some data can even be continuous, but measured in a discrete way (e.g., age). \n",
    "\n",
    "Likewise, `TotalDeaths` and `TotalReturns` in our `avengers` data are discrete variable.\n",
    "\n",
    "**Continuous data**\n",
    "\n",
    "Continuous data can take any value within a range: `Appearances` is an example in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Scaling numerical values\n",
    "\n",
    "Often times, the numeric variables in our dataset have very different scales, that is, take on different ranges of values.\n",
    "\n",
    "It's usually a good practice to scale them during the preprocessing of our data, typically you will do one of two things:\n",
    "1. Scale variables to a given range\n",
    "2. Standardize all variables.\n",
    "\n",
    "These transformations change the data itself, but not the distribution. Why is it important to scale the data:\n",
    "* When predictor values have different ranges, particular features can dominate the algorithm (e.g., think [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance))\n",
    "* Different scales can make estimators unable to learn correctly from certain features in smaller ranges\n",
    "* You don't want your feature to rely on the scale of the measurement involved\n",
    "* Optimization methods (e.g., gradient descent) will converge faster, and otherwise they may not converge at all.\n",
    "\n",
    "A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.\n",
    "\n",
    "**Scale all variables to a given range**\n",
    "\n",
    "We would transform all variables so that the minimum and the maximum of the transformed data take certain values, e.g., 0 and 1:\n",
    "\n",
    "$$ x_i' = \\frac{x_i - x_{min}}{x_{max} - x_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(df, scaler, plot=True):\n",
    "    df = df.copy()\n",
    "    cols = df.select_dtypes(include='int64').columns\n",
    "    df[cols] = scaler.fit_transform(df[cols])\n",
    "    if plot:\n",
    "        plot_scaled_data(df, cols)\n",
    "    return df\n",
    "\n",
    "def plot_scaled_data(df, cols):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for col in cols:\n",
    "        sns.distplot(df[col])\n",
    "    plt.title('Distribution of numerical variables (after scaling)')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "(avengers.pipe(scale_data, min_max_scaler)\n",
    "         .describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize all variables**\n",
    "\n",
    "Standardization means both centering the data around 0 (by removing the mean) and scaling it to unit variance:\n",
    "\n",
    "$$ z_i =  \\frac{x_i - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "(avengers.pipe(scale_data, standard_scaler)\n",
    "         .describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Categorical data\n",
    "\n",
    "Categorical data represents categories (e.g., gender, marital status, hometown).\n",
    "\n",
    "Categorical variables can take on a limited, and usually fixed, number of possible values, also known as levels.\n",
    "\n",
    "The categories can also take on numerical values (e.g., ids), but those numbers have no mathematical meaning:\n",
    "* You can't do mathematical operations, even if the computer says yes\n",
    "* Nor sort them in ascending or descending order.\n",
    "\n",
    "A limitation of categorical data in the form of strings is that estimators, in general, don't know how to deal with it.\n",
    "\n",
    "**Binary data**\n",
    "\n",
    "A binary variable is a variable with only two possible values: like `Active` and `Gender` in our `avengers` dataset.\n",
    "\n",
    "Since our algorithms can't deal with data in the form of strings, we need to transform such variables to a numerical form.\n",
    "\n",
    "The method `Series.map` allows us to easily deal with this cases, mapping inputs to outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avengers['Active'].map({'YES': 1, 'NO': 0})\n",
    "                   .sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use it convert both columns to either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avengers.assign(Active=avengers['Active'].map({'YES': 1, 'NO': 0}),\n",
    "                 Gender=avengers['Gender'].map({'MALE': 1, 'FEMALE': 0}))\n",
    "         .sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provide us with a `category` dtype for categorical data:\n",
    "* Easily identify and signal categorical columns for processing and other Python libraries\n",
    "* Converting a string variable with a few different values to a categorical variable saves memory\n",
    "* By converting to a categorical we can specify an order on the categories (more on this later).\n",
    "\n",
    "Let's consider a categorical features: `Universe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers_cat = avengers.copy()\n",
    "avengers_cat = avengers_cat.assign(Universe=avengers['Universe'].astype('category'))\n",
    "\n",
    "avengers_cat.describe(include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data has a `categories` and an `ordered` property:\n",
    "* `Series.cat.categories` prints the different values (or levels) the variable can take on\n",
    "* `Series.cat.ordered` prints whether the categorical variable has a natural order or not (hint: if it has, it's not purely categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers_cat['Universe'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers_cat['Universe'].cat.ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy encoding**\n",
    "\n",
    "Dummy encoding allows us to use categorical predictor variables in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = avengers.select_dtypes(include='category').columns\n",
    "(pd.get_dummies(avengers_cat, columns=categorical_features, drop_first=True).sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High cardinality data**\n",
    "\n",
    "The column `Name` is an example of a high cardinality categorical variable and try to dummify it, the *dimensionality* of the dataset will explode (more on this later).\n",
    "\n",
    "In fact, in this particular case, since if works as an identifier, we should simply drop it due to lack of relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers = avengers.drop('Name', axis=1)\n",
    "avengers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to deal with high cardinality would be to keep only the most frequent values, and encode the remaining ones as a special case (e.g., \"others\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Ordinal data\n",
    "\n",
    "Ordinal statistical data refers to categories that have a natural order, but the distance between them is not known.\n",
    "\n",
    "We will use the `Membership` variable as an example since it appears to be an order in the degree of commitment of our avengers.\n",
    "\n",
    "We can also use the `category` dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers_ord = avengers.copy()\n",
    "avengers_ord = avengers_ord.assign(Membership=avengers['Membership'].astype('category'))\n",
    "\n",
    "avengers_ord['Membership'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this time we need to set the order for our categories, since there is one! We `category` datatype is flexible enough to accommodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_cats = ['Honorary', 'Academy', 'Probationary', 'Full']\n",
    "avengers_ord = avengers_ord.assign(Membership=avengers_ord['Membership'].cat.set_categories(ordered_cats, ordered=True))\n",
    "\n",
    "avengers_ord['Membership'].min(), avengers_ord['Membership'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that our models need variables in numeric form, in order to be able to make sense of them.\n",
    "\n",
    "The `category` datatypes deals with this gracefully for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avengers_ord.assign(Membership=avengers_ord['Membership'].cat.codes)\n",
    "         .sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, and as usual, there is a trade-off here:\n",
    "* If we assign integer values to our ordinal categories we are imposing the assumption that they are equally spaced\n",
    "* If we convert them to dummy variables, we will lose the constraint with their order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bonus (not required for exercises)\n",
    "\n",
    "## 6.1 Scaling with outliers\n",
    "\n",
    "Scalers differ from each other in the way to estimate the parameters used to shift and scale each feature.\n",
    "\n",
    "In the presence of some very large outliers, using the scalers above leads to te compression of inliers:\n",
    "\n",
    "Since outliers have an influence on the minimum, maximum, mean and standard deviation, these scalers will *shrink* the range of the feature values.\n",
    "\n",
    "The alternative is to scale the features in a way that is robust to outliers: using the median (instead of the mean) and the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "standard_scaler = RobustScaler()\n",
    "(avengers.pipe(scale_data, standard_scaler)\n",
    "         .describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Other ways to encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are not processing the entire dataset at once, these encoders work great on preserving the encoding consistency:\n",
    "* The`.fit()` method assigns our categories or labels to a specific output (e.g., a numerical value)\n",
    "* Then`.transform()` transforms the data using this mapping, failing gracefully you when strange things happen (e.g., unseen categories).\n",
    "\n",
    "Also, they can used in very convenient ways with other `sklearn` utilities and a typical workflow.\n",
    "\n",
    "On the other hand, none of them is an option to deal with ordinal data (unless you decide to go with dummy encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(avengers['Universe'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to transform categories previously unseen by the encoder, it will raise an error (which is a good thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is supposed to go wrong :)\n",
    "le.transform(avengers['Membership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 One-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoder only accepts inputs in numerical form and typically we need to use it after the label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "universe_numeric = le.fit_transform(avengers['Universe'])\n",
    "he = OneHotEncoder(sparse=False, handle_unknown='error')\n",
    "he.fit_transform(universe_numeric.reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
